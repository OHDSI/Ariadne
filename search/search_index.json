{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Ariadne","text":"<p>Ariadne is a Python toolkit for mapping source terminologies to standard concepts in the OHDSI Vocabulary.</p> <p>It includes the following modules:</p> <ul> <li>evaluation: for evaluating mapping performance using golden standard mappings.</li> <li>llm_mapping: for using reasoning LLMs to find exact matches in the vocabulary.</li> <li>term_cleanup: for cleaning source terms, removing uninformative parts such as \"not otherwise specified\".</li> <li>utils: for common utility functions, including managing the configuration file.</li> <li>vector_search: for leveraging embedding language models to find semantically similar standard concepts for source terms.</li> <li>verbatim_mapping: for mapping source terms that (almost) exactly match standard concepts, using term normalization techniques like lowercasing, punctuation removal, and word stemming.</li> </ul>"},{"location":"autoapi/summary.html","title":"Summary","text":"<ul> <li>ariadne<ul> <li>evaluation<ul> <li>concept_search_evaluator</li> <li>concept_selection_evaluator</li> </ul> </li> <li>llm_mapping<ul> <li>concept_context_retriever</li> <li>llm_mapper</li> </ul> </li> <li>term_cleanup<ul> <li>term_cleaner</li> </ul> </li> <li>utils<ul> <li>config</li> <li>gen_ai_api</li> <li>logger</li> <li>utils</li> </ul> </li> <li>vector_search<ul> <li>abstract_concept_searcher</li> <li>hecate_concept_searcher</li> <li>pgvector_concept_searcher</li> </ul> </li> <li>verbatim_mapping<ul> <li>term_downloader</li> <li>term_normalizer</li> <li>verbatim_term_mapper</li> <li>vocab_verbatim_term_mapper</li> </ul> </li> </ul> </li> </ul>"},{"location":"autoapi/ariadne/index.html","title":"ariadne","text":""},{"location":"autoapi/ariadne/evaluation/index.html","title":"evaluation","text":""},{"location":"autoapi/ariadne/evaluation/concept_search_evaluator.html","title":"concept_search_evaluator","text":""},{"location":"autoapi/ariadne/evaluation/concept_search_evaluator.html#ariadne.evaluation.concept_search_evaluator.evaluate_concept_search","title":"<code>evaluate_concept_search(search_results, output_file, gold_standard_file='data/gold_standards/exact_matching_gs.csv', source_id_column='source_concept_id', term_column='cleaned_term', matched_concept_id_column='matched_concept_id', matched_concept_name_column='matched_concept_name', match_rank_column='match_rank')</code>","text":"<p>Evaluate the concept search results against the gold standard.</p> <p>Parameters:</p> Name Type Description Default <code>search_results</code> <code>DataFrame</code> <p>Pandas DataFrame containing the results of the concept search.</p> required <code>output_file</code> <code>str | Path</code> <p>Path to save the evaluation results.</p> required <code>gold_standard_file</code> <code>str</code> <p>Path to the CSV file containing the gold standard mappings.</p> <code>'data/gold_standards/exact_matching_gs.csv'</code> <code>source_id_column</code> <code>str</code> <p>Name of the column in the search results with source concept IDs.</p> <code>'source_concept_id'</code> <code>term_column</code> <code>str</code> <p>Name of the column in the search results with the search terms.</p> <code>'cleaned_term'</code> <code>matched_concept_id_column</code> <code>str</code> <p>Name of the column in the search results with matched concept IDs.</p> <code>'matched_concept_id'</code> <code>matched_concept_name_column</code> <code>str</code> <p>Name of the column in the search results with matched concept names.</p> <code>'matched_concept_name'</code> <code>match_rank_column</code> <code>str</code> <p>Name of the column in the search results with the rank of the matched concepts.</p> <code>'match_rank'</code> <p>Returns:</p> Type Description <code>None</code> <p>None. Execution results are written to the specified output file.</p> Source code in <code>src/ariadne/evaluation/concept_search_evaluator.py</code> <pre><code>def evaluate_concept_search(\n    search_results: pd.DataFrame,\n    output_file: str | Path,\n    gold_standard_file: str = \"data/gold_standards/exact_matching_gs.csv\",\n    source_id_column: str = \"source_concept_id\",\n    term_column: str = \"cleaned_term\",\n    matched_concept_id_column: str = \"matched_concept_id\",\n    matched_concept_name_column: str = \"matched_concept_name\",\n    match_rank_column: str = \"match_rank\",\n) -&gt; None:\n    \"\"\"\n    Evaluate the concept search results against the gold standard.\n\n    Args:\n        search_results: Pandas DataFrame containing the results of the concept search.\n        output_file: Path to save the evaluation results.\n        gold_standard_file: Path to the CSV file containing the gold standard mappings.\n        source_id_column: Name of the column in the search results with source concept IDs.\n        term_column: Name of the column in the search results with the search terms.\n        matched_concept_id_column: Name of the column in the search results with matched concept IDs.\n        matched_concept_name_column: Name of the column in the search results with matched concept names.\n        match_rank_column: Name of the column in the search results with the rank of the matched concepts.\n\n    Returns:\n        None. Execution results are written to the specified output file.\n    \"\"\"\n    detail_strings = []\n    # gold_standard = _load_gold_standard(resolve_path(gold_standard_file))\n    gold_standard = pd.read_csv(resolve_path(gold_standard_file))\n    evaluated_gs_count = 0\n    mean_average_precision = 0\n    recall_1 = 0\n    recall_3 = 0\n    recall_10 = 0\n    recall_25 = 0\n\n    grouped = search_results.groupby(source_id_column)\n    for source_id, group in grouped:\n        gs_entry = gold_standard[gold_standard[SOURCE_CONCEPT_ID] == source_id]\n        if gs_entry.empty:\n            continue\n        gs_entry = gs_entry.iloc[0]\n        gs_source_term = gs_entry[SOURCE_TERM]\n        gs_concept_id = gs_entry[TARGET_CONCEPT_ID]\n        gs_concept_id_b = gs_entry[TARGET_CONCEPT_ID_B]\n        gold_predicate = gs_entry[PREDICATE]\n        gold_predicate_b = gs_entry[PREDICATE_B]\n        if gold_predicate == BROAD_MATCH:\n            gs_concept_id = None\n        if gold_predicate_b == BROAD_MATCH:\n            gs_concept_id_b = None\n        if gs_concept_id is None and (gs_concept_id_b is None or math.isnan(gs_concept_id_b)):\n            continue\n        evaluated_gs_count = evaluated_gs_count + 1\n        gs_rank = group.loc[group[matched_concept_id_column] == gs_concept_id, match_rank_column]\n        if gs_concept_id_b is not None:\n            gs_rank_b = group.loc[group[matched_concept_id_column] == gs_concept_id_b, match_rank_column]\n            if not gs_rank_b.empty:\n                if gs_rank.empty or gs_rank_b.iloc[0] &lt; gs_rank.iloc[0]:\n                    gs_rank = gs_rank_b\n                    gs_concept_id = gs_concept_id_b\n        detail_strings.append(f\"Source term: {gs_source_term} ({source_id})\")\n        detail_strings.append(f\"Searched term: {group[term_column].iloc[0]}\")\n        if gs_rank.empty:\n            detail_strings.append(\"Gold standard concept not found\")\n            gs_concept_name = gs_entry[TARGET_CONCEPT_NAME]\n            detail_strings.append(f\"Correct target was: {gs_concept_name} ({gs_concept_id})\")\n        else:\n            gs_rank = gs_rank.iloc[0]\n            detail_strings.append(f\"Gold standard concept rank: {gs_rank}\")\n            mean_average_precision = mean_average_precision + 1 / gs_rank\n            if gs_rank &lt;= 1:\n                recall_1 = recall_1 + 1\n            if gs_rank &lt;= 3:\n                recall_3 = recall_3 + 1\n            if gs_rank &lt;= 10:\n                recall_10 = recall_10 + 1\n            if gs_rank &lt;= 25:\n                recall_25 = recall_25 + 1\n\n        detail_strings.append(\"\")\n\n        table = group[[match_rank_column, matched_concept_id_column, matched_concept_name_column]].copy()\n        correct = np.where(table[matched_concept_id_column] == gs_concept_id, \"Yes\", \"\")\n        if gs_concept_id_b:\n            correct_b = np.where(table[matched_concept_id_column] == gs_concept_id_b, \"Yes\", \"\")\n            correct = np.where(correct == \"Yes\", \"Yes\", correct_b)\n\n        table.insert(1, \"Correct\", correct)\n        detail_strings.append(table.to_string(index=False))\n        detail_strings.append(\"\")\n\n    mean_average_precision = mean_average_precision / evaluated_gs_count\n    recall_1 = recall_1 / evaluated_gs_count\n    recall_3 = recall_3 / evaluated_gs_count\n    recall_10 = recall_10 / evaluated_gs_count\n    recall_25 = recall_25 / evaluated_gs_count\n\n    summary_strings = [\n        f\"Evaluated gold standard concepts: {evaluated_gs_count}\",\n        f\"Mean Average Precision: {mean_average_precision}\",\n        f\"Recall@1: {recall_1}\",\n        f\"Recall@3: {recall_3}\",\n        f\"Recall@10: {recall_10}\",\n        f\"Recall@25: {recall_25}\",\n    ]\n\n    with open(output_file, \"w\", encoding=\"UTF-8\") as f:\n        f.write(\"\\n\".join(summary_strings))\n        f.write(\"\\n\\n\")\n        f.write(\"\\n\".join(detail_strings))\n        f.write(\"\\n\")\n\n    print(f\"Evaluation complete. Results written to {output_file}\")\n</code></pre>"},{"location":"autoapi/ariadne/evaluation/concept_selection_evaluator.html","title":"concept_selection_evaluator","text":""},{"location":"autoapi/ariadne/evaluation/concept_selection_evaluator.html#ariadne.evaluation.concept_selection_evaluator.evaluate","title":"<code>evaluate(selection_results, gold_standard_file='data/gold_standards/exact_matching_gs.csv', source_id_column='source_concept_id', term_column='cleaned_term', mapped_concept_id_column='mapped_concept_id', mapped_concept_name_column='mapped_concept_name', mapped_method_column='map_method', source_ids=None)</code>","text":"<p>Evaluate the concept selection results against the gold standard.</p> <p>Parameters:</p> Name Type Description Default <code>selection_results</code> <code>DataFrame</code> <p>Pandas DataFrame containing the results of selection results.</p> required <code>gold_standard_file</code> <code>str</code> <p>Path to the CSV file containing the gold standard mappings.</p> <code>'data/gold_standards/exact_matching_gs.csv'</code> <code>source_id_column</code> <code>str</code> <p>Name of the column with source concept IDs.</p> <code>'source_concept_id'</code> <code>term_column</code> <code>str</code> <p>Name of the column with source terms.</p> <code>'cleaned_term'</code> <code>mapped_concept_id_column</code> <code>str</code> <p>Name of the column with mapped concept IDs.</p> <code>'mapped_concept_id'</code> <code>mapped_concept_name_column</code> <code>str</code> <p>Name of the column with mapped concept names.</p> <code>'mapped_concept_name'</code> <code>mapped_method_column</code> <code>Optional[str]</code> <p>Optional: Name of the column with mapping methods, e.g. \"verbatim\" or \"llm\".</p> <code>'map_method'</code> <code>source_ids</code> <code>Optional[List[int]]</code> <p>Optional list of source concept IDs to evaluate. If None, evaluate all.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrame with the evaluation results.</p> Source code in <code>src/ariadne/evaluation/concept_selection_evaluator.py</code> <pre><code>def evaluate(\n    selection_results: pd.DataFrame,\n    gold_standard_file: str = \"data/gold_standards/exact_matching_gs.csv\",\n    source_id_column: str = \"source_concept_id\",\n    term_column: str = \"cleaned_term\",\n    mapped_concept_id_column: str = \"mapped_concept_id\",\n    mapped_concept_name_column: str = \"mapped_concept_name\",\n    mapped_method_column: Optional[str] = \"map_method\",\n    source_ids: Optional[List[int]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Evaluate the concept selection results against the gold standard.\n\n    Args:\n        selection_results: Pandas DataFrame containing the results of selection results.\n        gold_standard_file: Path to the CSV file containing the gold standard mappings.\n        source_id_column: Name of the column with source concept IDs.\n        term_column: Name of the column with source terms.\n        mapped_concept_id_column: Name of the column with mapped concept IDs.\n        mapped_concept_name_column: Name of the column with mapped concept names.\n        mapped_method_column: Optional: Name of the column with mapping methods, e.g. \"verbatim\" or \"llm\".\n        source_ids: Optional list of source concept IDs to evaluate. If None, evaluate all.\n\n    Returns:\n        A Pandas DataFrame with the evaluation results.\n    \"\"\"\n    gold_standard = pd.read_csv(resolve_path(gold_standard_file))\n\n    if mapped_method_column:\n        output_mapped_method_column = mapped_method_column\n    else:\n        output_mapped_method_column = \"map_method\"\n\n    selection_results.reset_index(drop=True, inplace=True)\n    evaluation_results = []\n    for index, row in selection_results.iterrows():\n        source_id = int(row[source_id_column])\n        if source_ids is not None and source_id not in source_ids:\n            continue\n\n        gold_entry = gold_standard[gold_standard[SOURCE_CONCEPT_ID] == source_id]\n        if gold_entry.empty:\n            continue\n        gold_entry = gold_entry.iloc[0]\n        gold_target_concept_id = gold_entry[TARGET_CONCEPT_ID]\n        gold_target_concept_id_b = gold_entry[TARGET_CONCEPT_ID_B]\n        gold_predicate = gold_entry[PREDICATE]\n        gold_predicate_b = gold_entry[PREDICATE_B]\n\n        mapped_concept_id = int(row[mapped_concept_id_column])\n        if mapped_method_column and mapped_method_column in row:\n            map_method = row[mapped_method_column]\n        else:\n            map_method = \"unknown\"\n        is_correct = (\n            (mapped_concept_id == gold_target_concept_id and gold_predicate == EXACT_MATCH)\n            or (mapped_concept_id == gold_target_concept_id_b and gold_predicate_b == EXACT_MATCH)\n            or (mapped_concept_id == -1 and gold_predicate == BROAD_MATCH)\n            or (mapped_concept_id == -1 and gold_predicate_b == BROAD_MATCH)\n        )\n        result_row = {\n            SOURCE_CONCEPT_ID: source_id,\n            SOURCE_TERM: gold_entry.get(SOURCE_TERM),\n            output_mapped_method_column: map_method,\n            TARGET_CONCEPT_ID: gold_target_concept_id,\n            TARGET_CONCEPT_NAME: gold_entry.get(TARGET_CONCEPT_NAME),\n            PREDICATE: gold_predicate,\n            TARGET_CONCEPT_ID_B: gold_target_concept_id_b,\n            TARGET_CONCEPT_NAME_B: gold_entry.get(TARGET_CONCEPT_NAME_B),\n            PREDICATE_B: gold_predicate_b,\n        }\n        if term_column != SOURCE_TERM:\n            result_row[term_column] = row[term_column]\n        result_row.update(\n            {\n                mapped_concept_id_column: mapped_concept_id,\n                mapped_concept_name_column: row[mapped_concept_name_column],\n                \"is_correct\": is_correct,\n            }\n        )\n        evaluation_results.append(result_row)\n    evaluation_df = pd.DataFrame(evaluation_results)\n\n    # Add overall accuracy as a column:\n    accuracy = evaluation_df[\"is_correct\"].mean()\n    evaluation_df[\"overall_accuracy\"] = accuracy\n\n    return evaluation_df\n</code></pre>"},{"location":"autoapi/ariadne/llm_mapping/index.html","title":"llm_mapping","text":""},{"location":"autoapi/ariadne/llm_mapping/concept_context_retriever.html","title":"concept_context_retriever","text":""},{"location":"autoapi/ariadne/llm_mapping/concept_context_retriever.html#ariadne.llm_mapping.concept_context_retriever.add_concept_context","title":"<code>add_concept_context(concept_table, concept_id_column='matched_concept_id', domain_id_column='matched_domain_id', concept_class_id_column='matched_concept_class_id', vocabulary_id_column='matched_vocabulary_id', add_parents=True, parents_column='matched_parents', add_children=True, children_column='matched_children', add_synonyms=True, synonyms_column='matched_synonyms')</code>","text":"<p>Adds concept context (domain, concept class, vocabulary, parents, children, synonyms) to the given concept table. Multiple entries per concept will be concatenated with semicolons. Children are limited to 10 random entries per concept.</p> <p>Parameters:</p> Name Type Description Default <code>concept_table</code> <code>DataFrame</code> <p>DataFrame containing concept IDs.</p> required <code>concept_id_column</code> <code>str</code> <p>Name of the column with concept IDs.</p> <code>'matched_concept_id'</code> <code>domain_id_column</code> <code>str</code> <p>Name of the column for the domain ID.</p> <code>'matched_domain_id'</code> <code>concept_class_id_column</code> <code>str</code> <p>Name of the column  for the domain concept class ID.</p> <code>'matched_concept_class_id'</code> <code>vocabulary_id_column</code> <code>str</code> <p>Name of the column  for the domain vocabulary ID.</p> <code>'matched_vocabulary_id'</code> <code>add_parents</code> <code>bool</code> <p>Whether to add parent concepts.</p> <code>True</code> <code>parents_column</code> <code>str</code> <p>Name of the column for parent concept names.</p> <code>'matched_parents'</code> <code>add_children</code> <code>bool</code> <p>Whether to add child concepts.</p> <code>True</code> <code>children_column</code> <code>str</code> <p>Name of the column for child concept names.</p> <code>'matched_children'</code> <code>add_synonyms</code> <code>bool</code> <p>Whether to add concept synonyms.</p> <code>True</code> <code>synonyms_column</code> <code>str</code> <p>Name of the column for concept synonyms.</p> <code>'matched_synonyms'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame enriched with concept context columns.</p> Source code in <code>src/ariadne/llm_mapping/concept_context_retriever.py</code> <pre><code>def add_concept_context(\n    concept_table: pd.DataFrame,\n    concept_id_column: str = \"matched_concept_id\",\n    domain_id_column: str = \"matched_domain_id\",\n    concept_class_id_column: str = \"matched_concept_class_id\",\n    vocabulary_id_column: str = \"matched_vocabulary_id\",\n    add_parents: bool = True,\n    parents_column: str = \"matched_parents\",\n    add_children: bool = True,\n    children_column: str = \"matched_children\",\n    add_synonyms: bool = True,\n    synonyms_column: str = \"matched_synonyms\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Adds concept context (domain, concept class, vocabulary, parents, children, synonyms) to the given concept table.\n    Multiple entries per concept will be concatenated with semicolons. Children are limited to 10 random entries per\n    concept.\n\n    Args:\n        concept_table: DataFrame containing concept IDs.\n        concept_id_column: Name of the column with concept IDs.\n        domain_id_column: Name of the column for the domain ID.\n        concept_class_id_column: Name of the column  for the domain concept class ID.\n        vocabulary_id_column: Name of the column  for the domain vocabulary ID.\n        add_parents:  Whether to add parent concepts.\n        parents_column: Name of the column for parent concept names.\n        add_children: Whether to add child concepts.\n        children_column: Name of the column for child concept names.\n        add_synonyms: Whether to add concept synonyms.\n        synonyms_column: Name of the column for concept synonyms.\n\n    Returns:\n        DataFrame enriched with concept context columns.\n    \"\"\"\n\n    engine = create_engine(get_environment_variable(\"VOCAB_CONNECTION_STRING\"))\n\n    concept_ids = concept_table[concept_id_column].unique().tolist()\n    query = _create_query(\n        concept_ids=concept_ids,\n        concept_class_id_column=concept_class_id_column,\n        domain_id_column=domain_id_column,\n        vocabulary_id_column=vocabulary_id_column,\n        add_parents=add_parents,\n        parents_column=parents_column,\n        add_children=add_children,\n        children_column=children_column,\n        add_synonyms=add_synonyms,\n        synonyms_column=synonyms_column,\n        engine=engine,\n    )\n\n    with engine.connect() as connection:\n        result = connection.execute(query)\n        context_df = pd.DataFrame(result.fetchall(), columns=result.keys())\n    merged_df = concept_table.merge(context_df, left_on=concept_id_column, right_on=\"concept_id\", how=\"left\")\n    merged_df.drop(columns=[\"concept_id\"], inplace=True)\n    return merged_df\n</code></pre>"},{"location":"autoapi/ariadne/llm_mapping/llm_mapper.html","title":"llm_mapper","text":""},{"location":"autoapi/ariadne/llm_mapping/llm_mapper.html#ariadne.llm_mapping.llm_mapper.LlmMapper","title":"<code>LlmMapper</code>","text":"Source code in <code>src/ariadne/llm_mapping/llm_mapper.py</code> <pre><code>class LlmMapper:\n    def __init__(self, config: Config = Config()):\n        self.system_prompts = config.llm_mapping.system_prompts\n        self.context_settings = config.llm_mapping.context\n        self.responses_folder = config.system.llm_mapper_responses_folder\n        os.makedirs(self.responses_folder, exist_ok=True)\n        self._cost = 0.0\n        \"\"\"\n        Initializes the LlmMapper with configuration settings, specific system prompts, and context settings for \n        LLM-based term mapping. Also sets up a folder to store LLM responses.\n        \"\"\"\n\n    def map_term(\n        self,\n        source_term: str,\n        source_id: Optional[str],\n        target_concepts: pd.DataFrame,\n        concept_id_column: str = \"matched_concept_id\",\n        concept_name_column: str = \"matched_concept_name\",\n        domain_id_column: Optional[str] = \"matched_domain_id\",\n        concept_class_id_column: Optional[str] = \"matched_concept_class_id\",\n        vocabulary_id_column: Optional[str] = \"matched_vocabulary_id\",\n        parents_column: Optional[str] = \"matched_parents\",\n        children_column: Optional[str] = \"matched_children\",\n        synonyms_column: Optional[str] = \"matched_synonyms\",\n    ) -&gt; Tuple[int, str, str]:\n        \"\"\"\n        Maps a source term to the matching target concept using LLM prompts. The LLM can be prompted in multiple\n        steps. The first step provides the source term and candidate target concepts as prompt, with information\n        specified in config.llm_mapping.context. Subsequent steps use the response from the previous step as prompt,\n        unless config.llm_mapping.context.re_insert_target_details is set to True, in which case the target concept\n        details are re-inserted into the response JSON for the next step.\n\n        Finally, the response is processed to extract the matched concept ID and name, looking for a line starting with\n        \"Match: &lt;concept_id&gt;\" or \"Match: no_match\".\n\n        Args:\n            source_term: The source clinical term to map.\n            source_id: An optional unique identifier for the source term, used for caching responses.\n            target_concepts: A DataFrame containing candidate target concepts with columns:\n            concept_id_column: The name of the column containing target concept IDs.\n            concept_name_column: The name of the column containing target concept names.\n            domain_id_column: The name of the column containing target domain IDs.\n            concept_class_id_column: The name of the column containing target concept class IDs.\n            vocabulary_id_column: The name of the column containing target vocabulary IDs.\n            parents_column: The name of the column containing target concept parents.\n            children_column: The name of the column containing target concept children.\n            synonyms_column: The name of the column containing target concept synonyms.\n\n        Returns:\n            A tuple of (matched_concept_id, matched_concept_name, match_rationale). If no match is found, returns\n            (-1, \"no_match\", \"\").\n        \"\"\"\n\n        num_prompts = len(self.system_prompts)\n        if source_id is None:\n            source_id = abs(hash(source_term)) % (10**8)\n\n        input_columns = [concept_id_column, concept_name_column]\n        context_columns = [\"concept_id\", \"concept_name\"]\n        if self.context_settings.include_target_class:\n            input_columns.append(concept_class_id_column)\n            context_columns.append(\"concept_class_id\")\n        if self.context_settings.include_target_parents:\n            input_columns.append(parents_column)\n            context_columns.append(\"concept_parents\")\n        if self.context_settings.include_target_domain:\n            input_columns.append(domain_id_column)\n            context_columns.append(\"concept_domain\")\n        if self.context_settings.include_target_vocabulary:\n            input_columns.append(vocabulary_id_column)\n            context_columns.append(\"concept_vocabulary\")\n        if self.context_settings.include_target_children:\n            input_columns.append(children_column)\n            context_columns.append(\"concept_children\")\n        if self.context_settings.include_target_synonyms:\n            input_columns.append(synonyms_column)\n            context_columns.append(\"concept_synonyms\")\n        context = target_concepts[input_columns]\n        context.columns = context_columns\n\n        prompt = \"\"\n        for step in range(num_prompts):\n            response_file = os.path.join(self.responses_folder, f\"response_{source_id}_s{step + 1}.txt\")\n\n            # Load response from file if it exists:\n            if os.path.exists(response_file):\n                with open(response_file, \"r\", encoding=\"utf-8\") as f:\n                    response = f.read()\n            else:\n                # Else generate a new response from the LLM:\n                system_prompt = self.system_prompts[step]\n                if step == 0:\n                    context_json = context.to_json(orient=\"records\", lines=True)\n                    prompt = f\"Source term: {source_term}\\n\\nCandidate target concepts:\\n{context_json}\"\n\n                response_with_usage = get_llm_response(prompt, system_prompt)\n                response = response_with_usage[\"content\"]\n                self._cost = self._cost + response_with_usage[\"usage\"][\"total_cost_usd\"]\n\n                if step == 0 and self.context_settings.re_insert_target_details:\n                    # Re-insert target details into the response JSON for the next step:\n                    response_json_match = re.search(r\"{.*}\", response, flags=re.DOTALL)\n                    if response_json_match:\n                        response_json_str = response_json_match.group(0)\n                        try:\n                            data = json.loads(response_json_str)\n                            target_definitions = data[\"target_concepts\"]\n                            target_definitions = pd.DataFrame(target_definitions)\n                            target_definitions[\"id\"] = pd.to_numeric(target_definitions[\"id\"], errors=\"coerce\")\n                            merged = pd.merge(\n                                target_definitions, context, left_on=\"id\", right_on=\"concept_id\", how=\"left\"\n                            )\n                            merged = merged.drop(columns=[\"concept_id\"])\n                            new_data = {\n                                \"source_term\": data[\"source_term\"],\n                                \"target_concepts\": merged.to_dict(orient=\"records\"),\n                            }\n                            response = json.dumps(new_data, indent=2)\n                        except Exception as e:\n                            print(f\"Warning: Could not re-insert target details: {e}\")\n\n                with open(response_file, \"w\", encoding=\"utf-8\") as f:\n                    f.write(response)\n            if step &lt; num_prompts - 1:\n                # Use the response as the prompt for the next step:\n                prompt = response\n\n        # Process the final response to extract the match:\n        match = re.findall(r\"^#+ ?Match ?:.*\", response, flags=re.MULTILINE | re.IGNORECASE)\n        if match:\n            if re.search(\"no[ _]match|-1\", match[-1], re.IGNORECASE):\n                match_value_int = -1\n                concept_name = \"no_match\"\n            else:\n                number_match = re.findall(r\"\\d+\", match[-1])\n                if not number_match:\n                    raise ValueError(f\"No numeric match found in response: {response}\")\n                number_match_value = number_match[0]\n                try:\n                    match_value_int = int(number_match_value)\n                except ValueError:\n                    raise ValueError(f\"Match value '{number_match_value}' is not a valid integer.\")\n                matched_row = target_concepts[target_concepts[concept_id_column] == match_value_int]\n                if matched_row.empty:\n                    raise ValueError(f\"Match '{number_match_value}' not found in search results.\")\n                concept_name = str(matched_row.iloc[0][concept_name_column])\n        elif re.search(r\":\\s?(no[ _]match|-1)\", match[-1], re.IGNORECASE):\n            match_value_int = -1\n            concept_name = \"no_match\"\n        else:\n            raise ValueError(\"Response does not contain a match or no_match.\")\n\n        # Extract the rationale if provided.\n        rationale_match = re.search(r\"Justification[:\\-]?(.*)\", response, flags=re.DOTALL | re.IGNORECASE)\n        rationale = \"\"\n        if rationale_match:\n            rationale = rationale_match.group(1).strip()\n            rationale = rationale.replace(\"\\n\", \" \").replace(\"\\\\n\", \"\\n\")\n\n        return match_value_int, concept_name, rationale\n\n    def map_terms(\n        self,\n        source_target_concepts: pd.DataFrame,\n        term_column: str = \"cleaned_term\",\n        source_id_column: Optional[str] = \"source_concept_id\",\n        source_term_column: Optional[str] = \"source_term\",\n        concept_id_column: str = \"matched_concept_id\",\n        concept_name_column: str = \"matched_concept_name\",\n        domain_id_column: Optional[str] = \"matched_domain_id\",\n        concept_class_id_column: Optional[str] = \"matched_concept_class_id\",\n        vocabulary_id_column: Optional[str] = \"matched_vocabulary_id\",\n        parents_column: Optional[str] = \"matched_parents\",\n        children_column: Optional[str] = \"matched_children\",\n        synonyms_column: Optional[str] = \"matched_synonyms\",\n        mapped_concept_id_column: str = \"mapped_concept_id\",\n        mapped_concept_name_column: str = \"mapped_concept_name\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps source terms in a DataFrame column to target concepts using LLM prompts. The system prompts are taken\n        from the configuration file. Multiple steps are supported as per the map_term method.\n\n        The input DataFrame should contain multiple rows per source term, one for each candidate target concept.\n\n        Be aware that LLM responses are cached based on source term and source ID, so if the same term appears\n        multiple times with the same source ID, the cached response will be used. The cache is stored in the\n        llm_mapper_responses_folder specified in the config.\n\n        Args:\n            source_target_concepts: DataFrame containing the source clinical terms and candidate target concepts.\n            term_column: The name of the column containing source terms fed to the LLM.\n            source_id_column: The name of the column containing the unique source term IDs.\n            source_term_column: The name of the column containing the original source terms.\n            concept_id_column: The name of the column containing the target concept IDs.\n            concept_name_column: The name of the column containing the target concept names.\n            domain_id_column: The name of the column containing the target domain IDs.\n            concept_class_id_column: The name of the column containing the target concept class IDs.\n            vocabulary_id_column: The name of the column containing the target vocabulary IDs.\n            parents_column: The name of the column containing the target concept parents.\n            children_column: The name of the column containing the target concept children.\n            synonyms_column: The name of the column containing the target concept synonyms.\n            mapped_concept_id_column: The name of the output column for mapped concept IDs.\n            mapped_concept_name_column: The name of the output column for mapped concept names.\n        Returns:\n            A DataFrame with the original terms and their mapped concept IDs and names.\n        \"\"\"\n\n        mapped_data = []\n        grouped = source_target_concepts.groupby(term_column)\n        for term, group in grouped:\n            source_id = None\n            if source_id_column and source_id_column in group.columns:\n                source_id = str(group.iloc[0][source_id_column])\n            matched_concept_id, matched_concept_name, match_rationale = self.map_term(\n                term,\n                source_id,\n                group,\n                concept_id_column,\n                concept_name_column,\n                domain_id_column,\n                concept_class_id_column,\n                vocabulary_id_column,\n                parents_column,\n                children_column,\n                synonyms_column,\n            )\n            mapped_data.append(\n                {\n                    term_column: term,\n                    source_id_column: source_id,\n                    source_term_column: group.iloc[0][source_term_column],\n                    mapped_concept_id_column: matched_concept_id,\n                    mapped_concept_name_column: matched_concept_name,\n                }\n            )\n        return pd.DataFrame(mapped_data)\n\n    def get_total_cost(self) -&gt; float:\n        \"\"\"\n        Returns the total cost incurred for LLM calls\n\n        Returns:\n            Total cost in USD.\n        \"\"\"\n\n        return self._cost\n</code></pre>"},{"location":"autoapi/ariadne/llm_mapping/llm_mapper.html#ariadne.llm_mapping.llm_mapper.LlmMapper.get_total_cost","title":"<code>get_total_cost()</code>","text":"<p>Returns the total cost incurred for LLM calls</p> <p>Returns:</p> Type Description <code>float</code> <p>Total cost in USD.</p> Source code in <code>src/ariadne/llm_mapping/llm_mapper.py</code> <pre><code>def get_total_cost(self) -&gt; float:\n    \"\"\"\n    Returns the total cost incurred for LLM calls\n\n    Returns:\n        Total cost in USD.\n    \"\"\"\n\n    return self._cost\n</code></pre>"},{"location":"autoapi/ariadne/llm_mapping/llm_mapper.html#ariadne.llm_mapping.llm_mapper.LlmMapper.map_term","title":"<code>map_term(source_term, source_id, target_concepts, concept_id_column='matched_concept_id', concept_name_column='matched_concept_name', domain_id_column='matched_domain_id', concept_class_id_column='matched_concept_class_id', vocabulary_id_column='matched_vocabulary_id', parents_column='matched_parents', children_column='matched_children', synonyms_column='matched_synonyms')</code>","text":"<p>Maps a source term to the matching target concept using LLM prompts. The LLM can be prompted in multiple steps. The first step provides the source term and candidate target concepts as prompt, with information specified in config.llm_mapping.context. Subsequent steps use the response from the previous step as prompt, unless config.llm_mapping.context.re_insert_target_details is set to True, in which case the target concept details are re-inserted into the response JSON for the next step.</p> <p>Finally, the response is processed to extract the matched concept ID and name, looking for a line starting with \"Match: \" or \"Match: no_match\". <p>Parameters:</p> Name Type Description Default <code>source_term</code> <code>str</code> <p>The source clinical term to map.</p> required <code>source_id</code> <code>Optional[str]</code> <p>An optional unique identifier for the source term, used for caching responses.</p> required <code>target_concepts</code> <code>DataFrame</code> <p>A DataFrame containing candidate target concepts with columns:</p> required <code>concept_id_column</code> <code>str</code> <p>The name of the column containing target concept IDs.</p> <code>'matched_concept_id'</code> <code>concept_name_column</code> <code>str</code> <p>The name of the column containing target concept names.</p> <code>'matched_concept_name'</code> <code>domain_id_column</code> <code>Optional[str]</code> <p>The name of the column containing target domain IDs.</p> <code>'matched_domain_id'</code> <code>concept_class_id_column</code> <code>Optional[str]</code> <p>The name of the column containing target concept class IDs.</p> <code>'matched_concept_class_id'</code> <code>vocabulary_id_column</code> <code>Optional[str]</code> <p>The name of the column containing target vocabulary IDs.</p> <code>'matched_vocabulary_id'</code> <code>parents_column</code> <code>Optional[str]</code> <p>The name of the column containing target concept parents.</p> <code>'matched_parents'</code> <code>children_column</code> <code>Optional[str]</code> <p>The name of the column containing target concept children.</p> <code>'matched_children'</code> <code>synonyms_column</code> <code>Optional[str]</code> <p>The name of the column containing target concept synonyms.</p> <code>'matched_synonyms'</code> <p>Returns:</p> Type Description <code>int</code> <p>A tuple of (matched_concept_id, matched_concept_name, match_rationale). If no match is found, returns</p> <code>str</code> <p>(-1, \"no_match\", \"\").</p> Source code in <code>src/ariadne/llm_mapping/llm_mapper.py</code> <pre><code>def map_term(\n    self,\n    source_term: str,\n    source_id: Optional[str],\n    target_concepts: pd.DataFrame,\n    concept_id_column: str = \"matched_concept_id\",\n    concept_name_column: str = \"matched_concept_name\",\n    domain_id_column: Optional[str] = \"matched_domain_id\",\n    concept_class_id_column: Optional[str] = \"matched_concept_class_id\",\n    vocabulary_id_column: Optional[str] = \"matched_vocabulary_id\",\n    parents_column: Optional[str] = \"matched_parents\",\n    children_column: Optional[str] = \"matched_children\",\n    synonyms_column: Optional[str] = \"matched_synonyms\",\n) -&gt; Tuple[int, str, str]:\n    \"\"\"\n    Maps a source term to the matching target concept using LLM prompts. The LLM can be prompted in multiple\n    steps. The first step provides the source term and candidate target concepts as prompt, with information\n    specified in config.llm_mapping.context. Subsequent steps use the response from the previous step as prompt,\n    unless config.llm_mapping.context.re_insert_target_details is set to True, in which case the target concept\n    details are re-inserted into the response JSON for the next step.\n\n    Finally, the response is processed to extract the matched concept ID and name, looking for a line starting with\n    \"Match: &lt;concept_id&gt;\" or \"Match: no_match\".\n\n    Args:\n        source_term: The source clinical term to map.\n        source_id: An optional unique identifier for the source term, used for caching responses.\n        target_concepts: A DataFrame containing candidate target concepts with columns:\n        concept_id_column: The name of the column containing target concept IDs.\n        concept_name_column: The name of the column containing target concept names.\n        domain_id_column: The name of the column containing target domain IDs.\n        concept_class_id_column: The name of the column containing target concept class IDs.\n        vocabulary_id_column: The name of the column containing target vocabulary IDs.\n        parents_column: The name of the column containing target concept parents.\n        children_column: The name of the column containing target concept children.\n        synonyms_column: The name of the column containing target concept synonyms.\n\n    Returns:\n        A tuple of (matched_concept_id, matched_concept_name, match_rationale). If no match is found, returns\n        (-1, \"no_match\", \"\").\n    \"\"\"\n\n    num_prompts = len(self.system_prompts)\n    if source_id is None:\n        source_id = abs(hash(source_term)) % (10**8)\n\n    input_columns = [concept_id_column, concept_name_column]\n    context_columns = [\"concept_id\", \"concept_name\"]\n    if self.context_settings.include_target_class:\n        input_columns.append(concept_class_id_column)\n        context_columns.append(\"concept_class_id\")\n    if self.context_settings.include_target_parents:\n        input_columns.append(parents_column)\n        context_columns.append(\"concept_parents\")\n    if self.context_settings.include_target_domain:\n        input_columns.append(domain_id_column)\n        context_columns.append(\"concept_domain\")\n    if self.context_settings.include_target_vocabulary:\n        input_columns.append(vocabulary_id_column)\n        context_columns.append(\"concept_vocabulary\")\n    if self.context_settings.include_target_children:\n        input_columns.append(children_column)\n        context_columns.append(\"concept_children\")\n    if self.context_settings.include_target_synonyms:\n        input_columns.append(synonyms_column)\n        context_columns.append(\"concept_synonyms\")\n    context = target_concepts[input_columns]\n    context.columns = context_columns\n\n    prompt = \"\"\n    for step in range(num_prompts):\n        response_file = os.path.join(self.responses_folder, f\"response_{source_id}_s{step + 1}.txt\")\n\n        # Load response from file if it exists:\n        if os.path.exists(response_file):\n            with open(response_file, \"r\", encoding=\"utf-8\") as f:\n                response = f.read()\n        else:\n            # Else generate a new response from the LLM:\n            system_prompt = self.system_prompts[step]\n            if step == 0:\n                context_json = context.to_json(orient=\"records\", lines=True)\n                prompt = f\"Source term: {source_term}\\n\\nCandidate target concepts:\\n{context_json}\"\n\n            response_with_usage = get_llm_response(prompt, system_prompt)\n            response = response_with_usage[\"content\"]\n            self._cost = self._cost + response_with_usage[\"usage\"][\"total_cost_usd\"]\n\n            if step == 0 and self.context_settings.re_insert_target_details:\n                # Re-insert target details into the response JSON for the next step:\n                response_json_match = re.search(r\"{.*}\", response, flags=re.DOTALL)\n                if response_json_match:\n                    response_json_str = response_json_match.group(0)\n                    try:\n                        data = json.loads(response_json_str)\n                        target_definitions = data[\"target_concepts\"]\n                        target_definitions = pd.DataFrame(target_definitions)\n                        target_definitions[\"id\"] = pd.to_numeric(target_definitions[\"id\"], errors=\"coerce\")\n                        merged = pd.merge(\n                            target_definitions, context, left_on=\"id\", right_on=\"concept_id\", how=\"left\"\n                        )\n                        merged = merged.drop(columns=[\"concept_id\"])\n                        new_data = {\n                            \"source_term\": data[\"source_term\"],\n                            \"target_concepts\": merged.to_dict(orient=\"records\"),\n                        }\n                        response = json.dumps(new_data, indent=2)\n                    except Exception as e:\n                        print(f\"Warning: Could not re-insert target details: {e}\")\n\n            with open(response_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(response)\n        if step &lt; num_prompts - 1:\n            # Use the response as the prompt for the next step:\n            prompt = response\n\n    # Process the final response to extract the match:\n    match = re.findall(r\"^#+ ?Match ?:.*\", response, flags=re.MULTILINE | re.IGNORECASE)\n    if match:\n        if re.search(\"no[ _]match|-1\", match[-1], re.IGNORECASE):\n            match_value_int = -1\n            concept_name = \"no_match\"\n        else:\n            number_match = re.findall(r\"\\d+\", match[-1])\n            if not number_match:\n                raise ValueError(f\"No numeric match found in response: {response}\")\n            number_match_value = number_match[0]\n            try:\n                match_value_int = int(number_match_value)\n            except ValueError:\n                raise ValueError(f\"Match value '{number_match_value}' is not a valid integer.\")\n            matched_row = target_concepts[target_concepts[concept_id_column] == match_value_int]\n            if matched_row.empty:\n                raise ValueError(f\"Match '{number_match_value}' not found in search results.\")\n            concept_name = str(matched_row.iloc[0][concept_name_column])\n    elif re.search(r\":\\s?(no[ _]match|-1)\", match[-1], re.IGNORECASE):\n        match_value_int = -1\n        concept_name = \"no_match\"\n    else:\n        raise ValueError(\"Response does not contain a match or no_match.\")\n\n    # Extract the rationale if provided.\n    rationale_match = re.search(r\"Justification[:\\-]?(.*)\", response, flags=re.DOTALL | re.IGNORECASE)\n    rationale = \"\"\n    if rationale_match:\n        rationale = rationale_match.group(1).strip()\n        rationale = rationale.replace(\"\\n\", \" \").replace(\"\\\\n\", \"\\n\")\n\n    return match_value_int, concept_name, rationale\n</code></pre>"},{"location":"autoapi/ariadne/llm_mapping/llm_mapper.html#ariadne.llm_mapping.llm_mapper.LlmMapper.map_terms","title":"<code>map_terms(source_target_concepts, term_column='cleaned_term', source_id_column='source_concept_id', source_term_column='source_term', concept_id_column='matched_concept_id', concept_name_column='matched_concept_name', domain_id_column='matched_domain_id', concept_class_id_column='matched_concept_class_id', vocabulary_id_column='matched_vocabulary_id', parents_column='matched_parents', children_column='matched_children', synonyms_column='matched_synonyms', mapped_concept_id_column='mapped_concept_id', mapped_concept_name_column='mapped_concept_name')</code>","text":"<p>Maps source terms in a DataFrame column to target concepts using LLM prompts. The system prompts are taken from the configuration file. Multiple steps are supported as per the map_term method.</p> <p>The input DataFrame should contain multiple rows per source term, one for each candidate target concept.</p> <p>Be aware that LLM responses are cached based on source term and source ID, so if the same term appears multiple times with the same source ID, the cached response will be used. The cache is stored in the llm_mapper_responses_folder specified in the config.</p> <p>Parameters:</p> Name Type Description Default <code>source_target_concepts</code> <code>DataFrame</code> <p>DataFrame containing the source clinical terms and candidate target concepts.</p> required <code>term_column</code> <code>str</code> <p>The name of the column containing source terms fed to the LLM.</p> <code>'cleaned_term'</code> <code>source_id_column</code> <code>Optional[str]</code> <p>The name of the column containing the unique source term IDs.</p> <code>'source_concept_id'</code> <code>source_term_column</code> <code>Optional[str]</code> <p>The name of the column containing the original source terms.</p> <code>'source_term'</code> <code>concept_id_column</code> <code>str</code> <p>The name of the column containing the target concept IDs.</p> <code>'matched_concept_id'</code> <code>concept_name_column</code> <code>str</code> <p>The name of the column containing the target concept names.</p> <code>'matched_concept_name'</code> <code>domain_id_column</code> <code>Optional[str]</code> <p>The name of the column containing the target domain IDs.</p> <code>'matched_domain_id'</code> <code>concept_class_id_column</code> <code>Optional[str]</code> <p>The name of the column containing the target concept class IDs.</p> <code>'matched_concept_class_id'</code> <code>vocabulary_id_column</code> <code>Optional[str]</code> <p>The name of the column containing the target vocabulary IDs.</p> <code>'matched_vocabulary_id'</code> <code>parents_column</code> <code>Optional[str]</code> <p>The name of the column containing the target concept parents.</p> <code>'matched_parents'</code> <code>children_column</code> <code>Optional[str]</code> <p>The name of the column containing the target concept children.</p> <code>'matched_children'</code> <code>synonyms_column</code> <code>Optional[str]</code> <p>The name of the column containing the target concept synonyms.</p> <code>'matched_synonyms'</code> <code>mapped_concept_id_column</code> <code>str</code> <p>The name of the output column for mapped concept IDs.</p> <code>'mapped_concept_id'</code> <code>mapped_concept_name_column</code> <code>str</code> <p>The name of the output column for mapped concept names.</p> <code>'mapped_concept_name'</code> <p>Returns:     A DataFrame with the original terms and their mapped concept IDs and names.</p> Source code in <code>src/ariadne/llm_mapping/llm_mapper.py</code> <pre><code>def map_terms(\n    self,\n    source_target_concepts: pd.DataFrame,\n    term_column: str = \"cleaned_term\",\n    source_id_column: Optional[str] = \"source_concept_id\",\n    source_term_column: Optional[str] = \"source_term\",\n    concept_id_column: str = \"matched_concept_id\",\n    concept_name_column: str = \"matched_concept_name\",\n    domain_id_column: Optional[str] = \"matched_domain_id\",\n    concept_class_id_column: Optional[str] = \"matched_concept_class_id\",\n    vocabulary_id_column: Optional[str] = \"matched_vocabulary_id\",\n    parents_column: Optional[str] = \"matched_parents\",\n    children_column: Optional[str] = \"matched_children\",\n    synonyms_column: Optional[str] = \"matched_synonyms\",\n    mapped_concept_id_column: str = \"mapped_concept_id\",\n    mapped_concept_name_column: str = \"mapped_concept_name\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps source terms in a DataFrame column to target concepts using LLM prompts. The system prompts are taken\n    from the configuration file. Multiple steps are supported as per the map_term method.\n\n    The input DataFrame should contain multiple rows per source term, one for each candidate target concept.\n\n    Be aware that LLM responses are cached based on source term and source ID, so if the same term appears\n    multiple times with the same source ID, the cached response will be used. The cache is stored in the\n    llm_mapper_responses_folder specified in the config.\n\n    Args:\n        source_target_concepts: DataFrame containing the source clinical terms and candidate target concepts.\n        term_column: The name of the column containing source terms fed to the LLM.\n        source_id_column: The name of the column containing the unique source term IDs.\n        source_term_column: The name of the column containing the original source terms.\n        concept_id_column: The name of the column containing the target concept IDs.\n        concept_name_column: The name of the column containing the target concept names.\n        domain_id_column: The name of the column containing the target domain IDs.\n        concept_class_id_column: The name of the column containing the target concept class IDs.\n        vocabulary_id_column: The name of the column containing the target vocabulary IDs.\n        parents_column: The name of the column containing the target concept parents.\n        children_column: The name of the column containing the target concept children.\n        synonyms_column: The name of the column containing the target concept synonyms.\n        mapped_concept_id_column: The name of the output column for mapped concept IDs.\n        mapped_concept_name_column: The name of the output column for mapped concept names.\n    Returns:\n        A DataFrame with the original terms and their mapped concept IDs and names.\n    \"\"\"\n\n    mapped_data = []\n    grouped = source_target_concepts.groupby(term_column)\n    for term, group in grouped:\n        source_id = None\n        if source_id_column and source_id_column in group.columns:\n            source_id = str(group.iloc[0][source_id_column])\n        matched_concept_id, matched_concept_name, match_rationale = self.map_term(\n            term,\n            source_id,\n            group,\n            concept_id_column,\n            concept_name_column,\n            domain_id_column,\n            concept_class_id_column,\n            vocabulary_id_column,\n            parents_column,\n            children_column,\n            synonyms_column,\n        )\n        mapped_data.append(\n            {\n                term_column: term,\n                source_id_column: source_id,\n                source_term_column: group.iloc[0][source_term_column],\n                mapped_concept_id_column: matched_concept_id,\n                mapped_concept_name_column: matched_concept_name,\n            }\n        )\n    return pd.DataFrame(mapped_data)\n</code></pre>"},{"location":"autoapi/ariadne/term_cleanup/index.html","title":"term_cleanup","text":""},{"location":"autoapi/ariadne/term_cleanup/term_cleaner.html","title":"term_cleaner","text":""},{"location":"autoapi/ariadne/term_cleanup/term_cleaner.html#ariadne.term_cleanup.term_cleaner.TermCleaner","title":"<code>TermCleaner</code>","text":"<p>A class to clean clinical terms by removing non-essential modifiers and information using a Large Language Model (LLM).</p> Source code in <code>src/ariadne/term_cleanup/term_cleaner.py</code> <pre><code>class TermCleaner:\n    \"\"\"\n    A class to clean clinical terms by removing non-essential modifiers and information using a Large Language Model (LLM).\n    \"\"\"\n\n    def __init__(self, config: Config = Config()):\n        self.system_prompt = config.term_cleaning.system_prompt\n        self.cost = 0.0\n\n    def clean_term(self, term: str) -&gt; str:\n        \"\"\"\n        Cleans a clinical term using an LLM to remove non-essential modifiers and information.\n\n        Args:\n            term: The clinical term to be cleaned.\n\n        Returns:\n            The cleaned clinical term.\n        \"\"\"\n\n        if re.search(_TRIGGER_PATTERN, term, flags=re.IGNORECASE) is None:\n            return term\n        prompt = f\"#Term: {term}\"\n        response = get_llm_response(prompt=prompt, system_prompt=self.system_prompt)\n        self.cost += response[\"usage\"][\"total_cost_usd\"]\n        pattern = r\"#Term: (.+)$\"\n        match = re.match(pattern, response[\"content\"].strip())\n        if match:\n            return match.group(1)  # Returns the captured answer\n        else:\n            warnings.warn(f\"Term {term} not found in response {response}\")\n            return term\n\n    def clean_terms(\n        self, df: pd.DataFrame, term_column: str = \"source_term\", output_column: str = \"cleaned_term\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Cleans clinical terms in a DataFrame column using the LLM.\n\n        Args:\n            df: DataFrame containing the terms to be cleaned.\n            term_column: Name of the column with terms to be cleaned.\n            output_column: Name of the column to store cleaned terms.\n\n        Returns:\n            DataFrame with an additional column for cleaned terms.\n        \"\"\"\n\n        df[output_column] = df[term_column].apply(self.clean_term)\n        return df\n\n    def get_total_cost(self) -&gt; float:\n        \"\"\"\n        Returns the total cost incurred for LLM calls during term cleaning.\n\n        Returns:\n            Total cost in USD.\n        \"\"\"\n\n        return self.cost\n</code></pre>"},{"location":"autoapi/ariadne/term_cleanup/term_cleaner.html#ariadne.term_cleanup.term_cleaner.TermCleaner.clean_term","title":"<code>clean_term(term)</code>","text":"<p>Cleans a clinical term using an LLM to remove non-essential modifiers and information.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>The clinical term to be cleaned.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The cleaned clinical term.</p> Source code in <code>src/ariadne/term_cleanup/term_cleaner.py</code> <pre><code>def clean_term(self, term: str) -&gt; str:\n    \"\"\"\n    Cleans a clinical term using an LLM to remove non-essential modifiers and information.\n\n    Args:\n        term: The clinical term to be cleaned.\n\n    Returns:\n        The cleaned clinical term.\n    \"\"\"\n\n    if re.search(_TRIGGER_PATTERN, term, flags=re.IGNORECASE) is None:\n        return term\n    prompt = f\"#Term: {term}\"\n    response = get_llm_response(prompt=prompt, system_prompt=self.system_prompt)\n    self.cost += response[\"usage\"][\"total_cost_usd\"]\n    pattern = r\"#Term: (.+)$\"\n    match = re.match(pattern, response[\"content\"].strip())\n    if match:\n        return match.group(1)  # Returns the captured answer\n    else:\n        warnings.warn(f\"Term {term} not found in response {response}\")\n        return term\n</code></pre>"},{"location":"autoapi/ariadne/term_cleanup/term_cleaner.html#ariadne.term_cleanup.term_cleaner.TermCleaner.clean_terms","title":"<code>clean_terms(df, term_column='source_term', output_column='cleaned_term')</code>","text":"<p>Cleans clinical terms in a DataFrame column using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the terms to be cleaned.</p> required <code>term_column</code> <code>str</code> <p>Name of the column with terms to be cleaned.</p> <code>'source_term'</code> <code>output_column</code> <code>str</code> <p>Name of the column to store cleaned terms.</p> <code>'cleaned_term'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with an additional column for cleaned terms.</p> Source code in <code>src/ariadne/term_cleanup/term_cleaner.py</code> <pre><code>def clean_terms(\n    self, df: pd.DataFrame, term_column: str = \"source_term\", output_column: str = \"cleaned_term\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Cleans clinical terms in a DataFrame column using the LLM.\n\n    Args:\n        df: DataFrame containing the terms to be cleaned.\n        term_column: Name of the column with terms to be cleaned.\n        output_column: Name of the column to store cleaned terms.\n\n    Returns:\n        DataFrame with an additional column for cleaned terms.\n    \"\"\"\n\n    df[output_column] = df[term_column].apply(self.clean_term)\n    return df\n</code></pre>"},{"location":"autoapi/ariadne/term_cleanup/term_cleaner.html#ariadne.term_cleanup.term_cleaner.TermCleaner.get_total_cost","title":"<code>get_total_cost()</code>","text":"<p>Returns the total cost incurred for LLM calls during term cleaning.</p> <p>Returns:</p> Type Description <code>float</code> <p>Total cost in USD.</p> Source code in <code>src/ariadne/term_cleanup/term_cleaner.py</code> <pre><code>def get_total_cost(self) -&gt; float:\n    \"\"\"\n    Returns the total cost incurred for LLM calls during term cleaning.\n\n    Returns:\n        Total cost in USD.\n    \"\"\"\n\n    return self.cost\n</code></pre>"},{"location":"autoapi/ariadne/utils/index.html","title":"utils","text":""},{"location":"autoapi/ariadne/utils/config.html","title":"config","text":""},{"location":"autoapi/ariadne/utils/config.html#ariadne.utils.config.Config","title":"<code>Config</code>","text":"<p>Configuration class for the Ariadne toolkit. Loads settings from a YAML file and provides structured access to configuration parameters.</p> Source code in <code>src/ariadne/utils/config.py</code> <pre><code>class Config:\n    \"\"\"\n    Configuration class for the Ariadne toolkit. Loads settings from a YAML file and provides structured access to\n    configuration parameters.\n    \"\"\"\n\n    system: SystemConfig = field(default_factory=SystemConfig)\n    verbatim_mapping: VerbatimMapping = field(default_factory=VerbatimMapping)\n    term_cleaning: TermCleaning = field(default_factory=TermCleaning)\n    vector_search: VectorSearch = field(default_factory=VectorSearch)\n    llm_mapping: Llm_mapping = field(default_factory=Llm_mapping)\n\n    def __init__(self, filename: str = \"config.yaml\"):\n        \"\"\"\n        Initializes the Config object by loading settings from the specified YAML file.\n\n        Args:\n            filename: The path to the YAML configuration file. Defaults to 'config.yaml' in the current working\n                        directory or project root.\n        \"\"\"\n\n        path = Path.cwd() / filename\n        if not path.exists():\n            path = get_project_root() / filename\n            if not path.exists():\n                raise FileNotFoundError(f\"Could not find {filename} in {Path.cwd()} or project root.\")\n        with path.open(\"r\", encoding=\"utf-8\") as fh:\n            raw = yaml.safe_load(fh) or {}\n\n        self.system = self.from_dict(SystemConfig, raw[\"system\"])\n        self.verbatim_mapping = self.from_dict(VerbatimMapping, raw[\"verbatim_mapping\"])\n        self.term_cleaning = self.from_dict(TermCleaning, raw[\"term_cleaning\"])\n        self.vector_search = self.from_dict(VectorSearch, raw[\"vector_search\"])\n        self.llm_mapping = self.from_dict(Llm_mapping, raw[\"llm_mapping\"])\n\n    def from_dict(self, cls: Type[\"Config\"], data: Dict[str, Any]) -&gt; \"Config\":\n        def build(dc_type: Type[Any], subdata: Dict[str, Any]) -&gt; Any:\n            if not is_dataclass(dc_type):\n                return subdata\n            kw = {}\n            for f in fields(dc_type):\n                if subdata is None or f.name not in subdata:\n                    continue\n                value = subdata[f.name]\n                if is_dataclass(f.type):\n                    kw[f.name] = build(f.type, value or {})\n                else:\n                    kw[f.name] = value\n            return dc_type(**kw)\n\n        return build(cls, data)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        def serialize(obj: Any) -&gt; Any:\n            if is_dataclass(obj):\n                result = {}\n                for f in fields(obj):\n                    value = getattr(obj, f.name)\n                    result[f.name] = serialize(value)\n                return result\n            elif isinstance(obj, list):\n                return [serialize(item) for item in obj]\n            else:\n                return obj\n\n        return {\n            \"system\": serialize(self.system),\n            \"verbatim_mapping\": serialize(self.verbatim_mapping),\n            \"term_cleaning\": serialize(self.term_cleaning),\n            \"vector_search\": serialize(self.vector_search),\n            \"llm_mapping\": serialize(self.llm_mapping),\n        }\n</code></pre>"},{"location":"autoapi/ariadne/utils/config.html#ariadne.utils.config.Config.__init__","title":"<code>__init__(filename='config.yaml')</code>","text":"<p>Initializes the Config object by loading settings from the specified YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the YAML configuration file. Defaults to 'config.yaml' in the current working         directory or project root.</p> <code>'config.yaml'</code> Source code in <code>src/ariadne/utils/config.py</code> <pre><code>def __init__(self, filename: str = \"config.yaml\"):\n    \"\"\"\n    Initializes the Config object by loading settings from the specified YAML file.\n\n    Args:\n        filename: The path to the YAML configuration file. Defaults to 'config.yaml' in the current working\n                    directory or project root.\n    \"\"\"\n\n    path = Path.cwd() / filename\n    if not path.exists():\n        path = get_project_root() / filename\n        if not path.exists():\n            raise FileNotFoundError(f\"Could not find {filename} in {Path.cwd()} or project root.\")\n    with path.open(\"r\", encoding=\"utf-8\") as fh:\n        raw = yaml.safe_load(fh) or {}\n\n    self.system = self.from_dict(SystemConfig, raw[\"system\"])\n    self.verbatim_mapping = self.from_dict(VerbatimMapping, raw[\"verbatim_mapping\"])\n    self.term_cleaning = self.from_dict(TermCleaning, raw[\"term_cleaning\"])\n    self.vector_search = self.from_dict(VectorSearch, raw[\"vector_search\"])\n    self.llm_mapping = self.from_dict(Llm_mapping, raw[\"llm_mapping\"])\n</code></pre>"},{"location":"autoapi/ariadne/utils/gen_ai_api.html","title":"gen_ai_api","text":""},{"location":"autoapi/ariadne/utils/gen_ai_api.html#ariadne.utils.gen_ai_api.get_embedding_vectors","title":"<code>get_embedding_vectors(texts)</code>","text":"<p>Generates embedding vectors for a list of texts using the embedding-specific config.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of texts to generate embeddings for.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary containing: - \"embeddings\": A numpy array of embedding vectors. - \"usage\": A dictionary with token usage and cost details.</p> Source code in <code>src/ariadne/utils/gen_ai_api.py</code> <pre><code>def get_embedding_vectors(texts: List[str]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generates embedding vectors for a list of texts using the embedding-specific config.\n\n    Args:\n        texts: List of texts to generate embeddings for.\n\n    Returns:\n        A dictionary containing:\n            - \"embeddings\": A numpy array of embedding vectors.\n            - \"usage\": A dictionary with token usage and cost details.\n    \"\"\"\n\n    client, model, provider = _AIClientFactory.get_client(task_type=\"embedding\")\n\n    response = client.embeddings.create(input=texts, model=model)\n\n    data = sorted(response.data, key=lambda x: x.index)\n    np_vectors = np.array([item.embedding for item in data])\n\n    usage = response.usage\n    total_cost = _calculate_cost(model, usage.prompt_tokens, 0, provider)\n\n    return {\n        \"embeddings\": np_vectors,\n        \"usage\": {\n            \"input_tokens\": usage.prompt_tokens,\n            \"output_tokens\": 0,\n            \"reasoning_tokens\": 0,\n            \"total_cost_usd\": total_cost,\n            \"model_used\": model,\n        },\n    }\n</code></pre>"},{"location":"autoapi/ariadne/utils/gen_ai_api.html#ariadne.utils.gen_ai_api.get_llm_response","title":"<code>get_llm_response(prompt, system_prompt=None)</code>","text":"<p>Generates text response using the LLM-specific config.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The user prompt to send to the LLM.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt to guide the LLM's behavior.</p> <code>None</code> <p>Returns:     A dictionary containing:         - \"content\": The generated text response from the LLM.         - \"usage\": A dictionary with token usage and cost details.</p> Source code in <code>src/ariadne/utils/gen_ai_api.py</code> <pre><code>def get_llm_response(\n    prompt: str, system_prompt: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Generates text response using the LLM-specific config.\n\n    Args:\n        prompt: The user prompt to send to the LLM.\n        system_prompt: Optional system prompt to guide the LLM's behavior.\n    Returns:\n        A dictionary containing:\n            - \"content\": The generated text response from the LLM.\n            - \"usage\": A dictionary with token usage and cost details.\n    \"\"\"\n\n    client, model, provider = _AIClientFactory.get_client(task_type=\"llm\")\n\n    messages = []\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    if model in _TEMPERATURE_OK_MODELS:\n        temperature = 0.0\n    else:\n        temperature = None\n\n    response = client.chat.completions.create(\n        model=model, messages=messages, temperature = temperature\n    )\n\n    usage = response.usage\n    reasoning_tokens = 0\n    if hasattr(usage, \"completion_tokens_details\") and usage.completion_tokens_details:\n        reasoning_tokens = getattr(\n            usage.completion_tokens_details, \"reasoning_tokens\", 0\n        )\n\n    total_cost = _calculate_cost(\n        model, usage.prompt_tokens, usage.completion_tokens, provider\n    )\n\n    return {\n        \"content\": response.choices[0].message.content,\n        \"usage\": {\n            \"input_tokens\": usage.prompt_tokens,\n            \"output_tokens\": usage.completion_tokens,\n            \"reasoning_tokens\": reasoning_tokens,\n            \"total_cost_usd\": total_cost,\n            \"model_used\": model,\n        },\n    }\n</code></pre>"},{"location":"autoapi/ariadne/utils/logger.html","title":"logger","text":""},{"location":"autoapi/ariadne/utils/logger.html#ariadne.utils.logger.open_log","title":"<code>open_log(log_file_name, clear_log_file=False)</code>","text":"<p>Sets up the root logger where it writes all logging events to file, and writing events at or above 'info' to console. Events are appended to the log file. The logger will also capture uncaught exceptions.</p> <p>Parameters:</p> Name Type Description Default <code>log_file_name</code> <code>str</code> <p>The name of the file where the log will be written to.</p> required <code>clear_log_file</code> <code>bool</code> <p>If true, the log file will be cleared before writing to it.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/ariadne/utils/logger.py</code> <pre><code>def open_log(log_file_name: str, clear_log_file: bool = False) -&gt; None:\n    \"\"\"\n    Sets up the root logger where it writes all logging events to file, and writing events at or above 'info' to\n    console. Events are appended to the log file. The logger will also capture uncaught exceptions.\n\n    Args:\n        log_file_name: The name of the file where the log will be written to.\n        clear_log_file: If true, the log file will be cleared before writing to it.\n\n    Returns:\n        None\n    \"\"\"\n\n    if clear_log_file:\n        open(log_file_name, \"w\").close()\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    if not len(logger.handlers):\n        _add_file_handler(logger=logger, log_file_name=log_file_name)\n        _add_stream_handler(logger=logger)\n\n    sys.excepthook = _handle_exception\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils.html","title":"utils","text":""},{"location":"autoapi/ariadne/utils/utils.html#ariadne.utils.utils.get_environment_variable","title":"<code>get_environment_variable(name)</code>","text":"<p>Retrieves the value of the specified environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the environment variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The value of the environment variable.</p> <p>Raises:     EnvironmentError: If the environment variable is not set.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def get_environment_variable(name: str) -&gt; str:\n    \"\"\"Retrieves the value of the specified environment variable.\n\n    Args:\n        name: The name of the environment variable.\n\n    Returns:\n        The value of the environment variable.\n    Raises:\n        EnvironmentError: If the environment variable is not set.\n    \"\"\"\n\n    value = os.getenv(name)\n    if value is None:\n        raise EnvironmentError(f\"Environment variable '{name}' is not set.\")\n    return value\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils.html#ariadne.utils.utils.get_project_root","title":"<code>get_project_root()</code>","text":"<p>Returns the path to the project root directory.</p> <p>Assumes this file is at src/ariadne/utils/config.py, so the root is 3 levels up.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def get_project_root() -&gt; Path:\n    \"\"\"Returns the path to the project root directory.\n\n    Assumes this file is at src/ariadne/utils/config.py,\n    so the root is 3 levels up.\n    \"\"\"\n\n    return Path(__file__).resolve().parent.parent.parent.parent\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils.html#ariadne.utils.utils.resolve_path","title":"<code>resolve_path(path)</code>","text":"<p>If the path is relative, makes it absolute by prepending the project root.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to resolve.</p> required <p>Returns:     The absolute file path as a string.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def resolve_path(path: str) -&gt; str:\n    \"\"\"If the path is relative, makes it absolute by prepending the project root.\n\n    Args:\n        path: The file path to resolve.\n    Returns:\n        The absolute file path as a string.\n    \"\"\"\n\n    p = Path(path)\n    if not p.is_absolute():\n        p = get_project_root() / p\n    return str(p.resolve())\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/index.html","title":"vector_search","text":""},{"location":"autoapi/ariadne/vector_search/abstract_concept_searcher.html","title":"abstract_concept_searcher","text":""},{"location":"autoapi/ariadne/vector_search/abstract_concept_searcher.html#ariadne.vector_search.abstract_concept_searcher.AbstractConceptSearcher","title":"<code>AbstractConceptSearcher</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for concept searchers.</p> Source code in <code>src/ariadne/vector_search/abstract_concept_searcher.py</code> <pre><code>class AbstractConceptSearcher(ABS):\n    \"\"\"Abstract base class for concept searchers.\"\"\"\n\n    @abstractmethod\n    def search_term(self, term: str, limit: int = 25) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Searches for concepts matching the given term.\n\n        Args:\n            term: The clinical term to search for.\n            limit: The maximum number of results to return.\n\n        Returns:\n            A DataFrame containing the matching concepts, or None if no matches are found.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def search_terms(\n        self,\n        df: pd.DataFrame,\n        term_column: str,\n        matched_concept_id_column: str = \"matched_concept_id\",\n        matched_concept_name_column: str = \"matched_concept_name\",\n        match_score_column: str = \"match_score\",\n        match_rank_column: str = \"match_rank\",\n        limit: int = 25,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Searches for concepts matching terms in a DataFrame column.\n\n        Args:\n            df: DataFrame containing the terms to search for.\n            term_column: Name of the column with terms to search.\n            matched_concept_id_column: Name of the column to store matched concept IDs.\n            matched_concept_name_column: Name of the column to store matched concept names.\n            match_score_column: Name of the column to store match scores.\n            match_rank_column: Name of the column to store match ranks.\n            limit: The maximum number of results to return for each term.\n\n        Returns:\n            A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For\n            each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/abstract_concept_searcher.html#ariadne.vector_search.abstract_concept_searcher.AbstractConceptSearcher.search_term","title":"<code>search_term(term, limit=25)</code>  <code>abstractmethod</code>","text":"<p>Searches for concepts matching the given term.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>The clinical term to search for.</p> required <code>limit</code> <code>int</code> <p>The maximum number of results to return.</p> <code>25</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>A DataFrame containing the matching concepts, or None if no matches are found.</p> Source code in <code>src/ariadne/vector_search/abstract_concept_searcher.py</code> <pre><code>@abstractmethod\ndef search_term(self, term: str, limit: int = 25) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Searches for concepts matching the given term.\n\n    Args:\n        term: The clinical term to search for.\n        limit: The maximum number of results to return.\n\n    Returns:\n        A DataFrame containing the matching concepts, or None if no matches are found.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/abstract_concept_searcher.html#ariadne.vector_search.abstract_concept_searcher.AbstractConceptSearcher.search_terms","title":"<code>search_terms(df, term_column, matched_concept_id_column='matched_concept_id', matched_concept_name_column='matched_concept_name', match_score_column='match_score', match_rank_column='match_rank', limit=25)</code>  <code>abstractmethod</code>","text":"<p>Searches for concepts matching terms in a DataFrame column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the terms to search for.</p> required <code>term_column</code> <code>str</code> <p>Name of the column with terms to search.</p> required <code>matched_concept_id_column</code> <code>str</code> <p>Name of the column to store matched concept IDs.</p> <code>'matched_concept_id'</code> <code>matched_concept_name_column</code> <code>str</code> <p>Name of the column to store matched concept names.</p> <code>'matched_concept_name'</code> <code>match_score_column</code> <code>str</code> <p>Name of the column to store match scores.</p> <code>'match_score'</code> <code>match_rank_column</code> <code>str</code> <p>Name of the column to store match ranks.</p> <code>'match_rank'</code> <code>limit</code> <code>int</code> <p>The maximum number of results to return for each term.</p> <code>25</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For</p> <code>DataFrame</code> <p>each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.</p> Source code in <code>src/ariadne/vector_search/abstract_concept_searcher.py</code> <pre><code>@abstractmethod\ndef search_terms(\n    self,\n    df: pd.DataFrame,\n    term_column: str,\n    matched_concept_id_column: str = \"matched_concept_id\",\n    matched_concept_name_column: str = \"matched_concept_name\",\n    match_score_column: str = \"match_score\",\n    match_rank_column: str = \"match_rank\",\n    limit: int = 25,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Searches for concepts matching terms in a DataFrame column.\n\n    Args:\n        df: DataFrame containing the terms to search for.\n        term_column: Name of the column with terms to search.\n        matched_concept_id_column: Name of the column to store matched concept IDs.\n        matched_concept_name_column: Name of the column to store matched concept names.\n        match_score_column: Name of the column to store match scores.\n        match_rank_column: Name of the column to store match ranks.\n        limit: The maximum number of results to return for each term.\n\n    Returns:\n        A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For\n        each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/hecate_concept_searcher.html","title":"hecate_concept_searcher","text":""},{"location":"autoapi/ariadne/vector_search/hecate_concept_searcher.html#ariadne.vector_search.hecate_concept_searcher.HecateConceptSearcher","title":"<code>HecateConceptSearcher</code>","text":"<p>               Bases: <code>AbstractConceptSearcher</code></p> <p>A concept searcher that uses the OHDSI Hecate API to find concepts based on query strings.</p> Source code in <code>src/ariadne/vector_search/hecate_concept_searcher.py</code> <pre><code>class HecateConceptSearcher(AbstractConceptSearcher):\n\n    \"\"\"\n    A concept searcher that uses the OHDSI Hecate API to find concepts based on query strings.\n    \"\"\"\n\n    def __init__(self, for_evaluation: bool = False):\n        \"\"\"\n        Initializes the HecateConceptSearcher.\n\n        Args:\n            for_evaluation: If True, configures the searcher for evaluation purposes.\n        \"\"\"\n        self.for_evaluation = for_evaluation\n\n        if for_evaluation:\n            print(\"HecateConceptSearcher initialized in evaluation mode.\")\n            self.default_params = {\n                \"standard_concept\": \"S\",\n                \"domain_id\": \"Condition,Observation,Measurement,Procedure\",\n                \"concept_class_id\": \"3-dig billing code,3-dig nonbill code,4-dig billing code,Answer,Claims Attachment,Clinical Finding,Clinical Observation,Context-dependent,CPT4,CPT4 Modifier,Disorder,Event,Genetic Variation,HCPCS,Histopattern,ICD10PCS,ICD10PCS Hierarchy,ICDO Condition,ICDO Histology,Ingredient,Lab Test,MDC,Metastasis,MS-DRG,NAACCR Variable,Observable Entity,Procedure,Question,Social Context,Staging / Scales,Staging/Grading,Survey,Topic,Topography,Value,Variable\",\n                \"exclude_vocabulary_id\": \"ICD9CM,ICD10CM,ICD10,ICD10CN,ICD10GM,CIM10,ICDO3,KCD7,Read\",\n            }\n        else:\n            print(\"HecateConceptSearcher initialized in standard mode.\")\n            self.default_params = {\n                \"standard_concept\": \"S\",\n            }\n\n    def search_term(self, query_string: str, limit: int = 25) -&gt; DataFrame:\n        \"\"\"\n        Searches for concepts matching the given query string.\n\n        Args:\n            query_string: The term to search for.\n            limit: The maximum number of results to return.\n\n        Returns:\n            A DataFrame containing the matching concepts, with the same columns as the concept table in the OMOP CDM,\n            plus a 'score' column indicating the relevance score from the search.\n\n        \"\"\"\n\n        params = {\"q\": query_string, \"limit\": limit}\n        params.update(self.default_params)\n\n        try:\n            response = requests.get(_HECATE_URL, params=params, timeout=15)\n            response.raise_for_status()\n            terms = response.json()\n            concepts = []\n            for term in terms:\n                # add score:\n                for concept in term.get(\"concepts\", []):\n                    concept[\"score\"] = term.get(\"score\", None)\n                concepts.extend(term.get(\"concepts\", []))\n            return pd.DataFrame(concepts)\n\n        except requests.exceptions.HTTPError as http_err:\n            print(f\"HTTP error occurred: {http_err}\")\n            print(f\"Response status code: {response.status_code}\")\n            print(f\"Response content: {response.text}\")\n        except requests.exceptions.ConnectionError as conn_err:\n            print(f\"Connection error occurred: {conn_err}\")\n        except requests.exceptions.Timeout as timeout_err:\n            print(f\"The request timed out: {timeout_err}\")\n        except requests.exceptions.RequestException as err:\n            print(f\"An unexpected error occurred: {err}\")\n\n        return None\n\n    def search_terms(\n        self,\n        df: pd.DataFrame,\n        term_column: str,\n        matched_concept_id_column: str = \"matched_concept_id\",\n        matched_concept_name_column: str = \"matched_concept_name\",\n        match_score_column: str = \"match_score\",\n        match_rank_column: str = \"match_rank\",\n        limit: int = 25,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Searches the Hecate API for concepts matching terms in a DataFrame column.\n\n        Args:\n            df: DataFrame containing the terms to search for.\n            term_column: Name of the column with terms to search.\n            matched_concept_id_column: Name of the column to store matched concept IDs.\n            matched_concept_name_column: Name of the column to store matched concept names.\n            match_score_column: Name of the column to store match scores.\n            match_rank_column: Name of the column to store match ranks.\n            limit: The maximum number of results to return for each term.\n\n        Returns:\n            A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For\n            each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.\n\n        \"\"\"\n\n        all_results = []\n        for index, row in df.iterrows():\n            term = row[term_column]\n            print(f\"Processing term '{term}'\")\n            results = self.search_term(term, limit=limit)\n            if results is not None:\n                rows = []\n                for rank, (_, concept) in enumerate(results.iterrows(), start=1):\n                    rows.append(\n                        {\n                            matched_concept_id_column: concept[\"concept_id\"],\n                            matched_concept_name_column: concept[\"concept_name\"],\n                            match_score_column: concept[\"score\"],\n                            match_rank_column: rank,\n                        }\n                    )\n                results = pd.DataFrame(rows)\n                results[match_rank_column] = range(1, len(results) + 1)\n                orig_cols = list(df.columns)\n                new_columns = list(results.columns)\n                results[term_column] = term\n                for col in df.columns:\n                    results[col] = row[col]\n                results = results[orig_cols + new_columns]\n                all_results.append(results)\n\n        all_results = pd.concat(all_results)\n        return all_results\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/hecate_concept_searcher.html#ariadne.vector_search.hecate_concept_searcher.HecateConceptSearcher.__init__","title":"<code>__init__(for_evaluation=False)</code>","text":"<p>Initializes the HecateConceptSearcher.</p> <p>Parameters:</p> Name Type Description Default <code>for_evaluation</code> <code>bool</code> <p>If True, configures the searcher for evaluation purposes.</p> <code>False</code> Source code in <code>src/ariadne/vector_search/hecate_concept_searcher.py</code> <pre><code>def __init__(self, for_evaluation: bool = False):\n    \"\"\"\n    Initializes the HecateConceptSearcher.\n\n    Args:\n        for_evaluation: If True, configures the searcher for evaluation purposes.\n    \"\"\"\n    self.for_evaluation = for_evaluation\n\n    if for_evaluation:\n        print(\"HecateConceptSearcher initialized in evaluation mode.\")\n        self.default_params = {\n            \"standard_concept\": \"S\",\n            \"domain_id\": \"Condition,Observation,Measurement,Procedure\",\n            \"concept_class_id\": \"3-dig billing code,3-dig nonbill code,4-dig billing code,Answer,Claims Attachment,Clinical Finding,Clinical Observation,Context-dependent,CPT4,CPT4 Modifier,Disorder,Event,Genetic Variation,HCPCS,Histopattern,ICD10PCS,ICD10PCS Hierarchy,ICDO Condition,ICDO Histology,Ingredient,Lab Test,MDC,Metastasis,MS-DRG,NAACCR Variable,Observable Entity,Procedure,Question,Social Context,Staging / Scales,Staging/Grading,Survey,Topic,Topography,Value,Variable\",\n            \"exclude_vocabulary_id\": \"ICD9CM,ICD10CM,ICD10,ICD10CN,ICD10GM,CIM10,ICDO3,KCD7,Read\",\n        }\n    else:\n        print(\"HecateConceptSearcher initialized in standard mode.\")\n        self.default_params = {\n            \"standard_concept\": \"S\",\n        }\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/hecate_concept_searcher.html#ariadne.vector_search.hecate_concept_searcher.HecateConceptSearcher.search_term","title":"<code>search_term(query_string, limit=25)</code>","text":"<p>Searches for concepts matching the given query string.</p> <p>Parameters:</p> Name Type Description Default <code>query_string</code> <code>str</code> <p>The term to search for.</p> required <code>limit</code> <code>int</code> <p>The maximum number of results to return.</p> <code>25</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the matching concepts, with the same columns as the concept table in the OMOP CDM,</p> <code>DataFrame</code> <p>plus a 'score' column indicating the relevance score from the search.</p> Source code in <code>src/ariadne/vector_search/hecate_concept_searcher.py</code> <pre><code>def search_term(self, query_string: str, limit: int = 25) -&gt; DataFrame:\n    \"\"\"\n    Searches for concepts matching the given query string.\n\n    Args:\n        query_string: The term to search for.\n        limit: The maximum number of results to return.\n\n    Returns:\n        A DataFrame containing the matching concepts, with the same columns as the concept table in the OMOP CDM,\n        plus a 'score' column indicating the relevance score from the search.\n\n    \"\"\"\n\n    params = {\"q\": query_string, \"limit\": limit}\n    params.update(self.default_params)\n\n    try:\n        response = requests.get(_HECATE_URL, params=params, timeout=15)\n        response.raise_for_status()\n        terms = response.json()\n        concepts = []\n        for term in terms:\n            # add score:\n            for concept in term.get(\"concepts\", []):\n                concept[\"score\"] = term.get(\"score\", None)\n            concepts.extend(term.get(\"concepts\", []))\n        return pd.DataFrame(concepts)\n\n    except requests.exceptions.HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n        print(f\"Response status code: {response.status_code}\")\n        print(f\"Response content: {response.text}\")\n    except requests.exceptions.ConnectionError as conn_err:\n        print(f\"Connection error occurred: {conn_err}\")\n    except requests.exceptions.Timeout as timeout_err:\n        print(f\"The request timed out: {timeout_err}\")\n    except requests.exceptions.RequestException as err:\n        print(f\"An unexpected error occurred: {err}\")\n\n    return None\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/hecate_concept_searcher.html#ariadne.vector_search.hecate_concept_searcher.HecateConceptSearcher.search_terms","title":"<code>search_terms(df, term_column, matched_concept_id_column='matched_concept_id', matched_concept_name_column='matched_concept_name', match_score_column='match_score', match_rank_column='match_rank', limit=25)</code>","text":"<p>Searches the Hecate API for concepts matching terms in a DataFrame column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the terms to search for.</p> required <code>term_column</code> <code>str</code> <p>Name of the column with terms to search.</p> required <code>matched_concept_id_column</code> <code>str</code> <p>Name of the column to store matched concept IDs.</p> <code>'matched_concept_id'</code> <code>matched_concept_name_column</code> <code>str</code> <p>Name of the column to store matched concept names.</p> <code>'matched_concept_name'</code> <code>match_score_column</code> <code>str</code> <p>Name of the column to store match scores.</p> <code>'match_score'</code> <code>match_rank_column</code> <code>str</code> <p>Name of the column to store match ranks.</p> <code>'match_rank'</code> <code>limit</code> <code>int</code> <p>The maximum number of results to return for each term.</p> <code>25</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For</p> <code>DataFrame</code> <p>each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.</p> Source code in <code>src/ariadne/vector_search/hecate_concept_searcher.py</code> <pre><code>def search_terms(\n    self,\n    df: pd.DataFrame,\n    term_column: str,\n    matched_concept_id_column: str = \"matched_concept_id\",\n    matched_concept_name_column: str = \"matched_concept_name\",\n    match_score_column: str = \"match_score\",\n    match_rank_column: str = \"match_rank\",\n    limit: int = 25,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Searches the Hecate API for concepts matching terms in a DataFrame column.\n\n    Args:\n        df: DataFrame containing the terms to search for.\n        term_column: Name of the column with terms to search.\n        matched_concept_id_column: Name of the column to store matched concept IDs.\n        matched_concept_name_column: Name of the column to store matched concept names.\n        match_score_column: Name of the column to store match scores.\n        match_rank_column: Name of the column to store match ranks.\n        limit: The maximum number of results to return for each term.\n\n    Returns:\n        A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For\n        each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.\n\n    \"\"\"\n\n    all_results = []\n    for index, row in df.iterrows():\n        term = row[term_column]\n        print(f\"Processing term '{term}'\")\n        results = self.search_term(term, limit=limit)\n        if results is not None:\n            rows = []\n            for rank, (_, concept) in enumerate(results.iterrows(), start=1):\n                rows.append(\n                    {\n                        matched_concept_id_column: concept[\"concept_id\"],\n                        matched_concept_name_column: concept[\"concept_name\"],\n                        match_score_column: concept[\"score\"],\n                        match_rank_column: rank,\n                    }\n                )\n            results = pd.DataFrame(rows)\n            results[match_rank_column] = range(1, len(results) + 1)\n            orig_cols = list(df.columns)\n            new_columns = list(results.columns)\n            results[term_column] = term\n            for col in df.columns:\n                results[col] = row[col]\n            results = results[orig_cols + new_columns]\n            all_results.append(results)\n\n    all_results = pd.concat(all_results)\n    return all_results\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/pgvector_concept_searcher.html","title":"pgvector_concept_searcher","text":""},{"location":"autoapi/ariadne/vector_search/pgvector_concept_searcher.html#ariadne.vector_search.pgvector_concept_searcher.PgvectorConceptSearcher","title":"<code>PgvectorConceptSearcher</code>","text":"<p>               Bases: <code>AbstractConceptSearcher</code></p> <p>A concept searcher that uses pgvector in a PostgreSQL database to find concepts based on embedding vectors.</p> Source code in <code>src/ariadne/vector_search/pgvector_concept_searcher.py</code> <pre><code>class PgvectorConceptSearcher(AbstractConceptSearcher):\n\n    \"\"\"\n    A concept searcher that uses pgvector in a PostgreSQL database to find concepts based on embedding vectors.\n    \"\"\"\n\n    def __init__(self, for_evaluation: bool = False, include_synonyms: bool = True, include_mapped_terms: bool = True):\n        self.for_evaluation = for_evaluation\n        self.include_synonyms = include_synonyms\n        self.include_mapped_terms = include_mapped_terms\n        self.cost = 0.0\n\n        if for_evaluation:\n            self.concept_classes_to_ignore = [\n                \"Disposition\",\n                \"Morph Abnormality\",\n                \"Organism\",\n                \"Qualifier Value\",\n                \"Substance\",\n                \"ICDO Condition\",\n            ]\n            self.vocabularies_to_ignore = [\n                \"ICD9CM\",\n                \"ICD10CM\",\n                \"ICD10\",\n                \"ICD10CN\",\n                \"ICD10GM\",\n                \"CIM10\",\n                \"ICDO3\",\n                \"KCD7\",\n                \"Read\",\n            ]\n            print(\"ConceptSearcher initialized in evaluation mode.\")\n        else:\n            self.concept_classes_to_ignore = None\n            self.vocabularies_to_ignore = None\n\n        connection = psycopg.connect(os.getenv(\"vocab_connection_string\").replace(\"+psycopg\", \"\"))\n        register_vector(connection)\n        with connection.cursor() as cur:\n            cur.execute(\"SET hnsw.ef_search = 1000\")\n            cur.execute(\"SET hnsw.iterative_scan = relaxed_order\")\n        self.connection = connection\n\n    def close(self):\n        self.connection.close()\n\n    def _search_pgvector(self, source_vector: np.ndarray, limit: int) -&gt; List:\n        if self.concept_classes_to_ignore is None:\n            ignore_string_class = \"'dummy'\"\n        else:\n            ignore_string_class = \", \".join(f\"'{y}'\" for y in self.concept_classes_to_ignore)\n\n        if self.include_synonyms:\n            term_type_clause = \"\"\n        else:\n            term_type_clause = \"AND vectors.term_type = 'Name'\"\n\n        vocabulary_schema = get_environment_variable(\"VOCAB_SCHEMA\")\n        vector_table = get_environment_variable(\"VOCAB_VECTOR_TABLE\")\n\n        if self.include_mapped_terms:\n            if self.vocabularies_to_ignore is None:\n                ignore_string_vocab = \"'dummy'\"\n            else:\n                ignore_string_vocab = \", \".join(f\"'{x}'\" for x in self.vocabularies_to_ignore)\n            query = f\"\"\"\n                WITH target_concept AS (\n                    SELECT concept_id,\n                        concept_name,\n                        MIN(relevance_score) AS relevance_score\n                    FROM (\n                        (\n                            SELECT concept.concept_id,\n                                concept.concept_name,\n                                embedding_vector &lt;=&gt; %s AS relevance_score\n                            FROM {vocabulary_schema}.{vector_table} vectors\n                            INNER JOIN {vocabulary_schema}.concept source_concept\n                                ON vectors.concept_id = source_concept.concept_id\n                            INNER JOIN {vocabulary_schema}.concept_relationship\n                                ON vectors.concept_id = concept_relationship.concept_id_1\n                            INNER JOIN {vocabulary_schema}.concept\n                                ON concept_relationship.concept_id_2 = concept.concept_id\n                            WHERE relationship_id = 'Maps to'\n                                AND source_concept.vocabulary_id NOT IN ({ignore_string_vocab})\n                                AND concept.concept_class_id NOT IN ({ignore_string_class})\n                                {term_type_clause}\n                            ORDER BY embedding_vector &lt;=&gt; %s\n                            LIMIT {limit * 4} -- May have duplicates due to synonyms\n                        )\n\n                        UNION ALL\n\n                        (\n                            SELECT concept.concept_id,\n                                concept.concept_name,\n                                embedding_vector &lt;=&gt; %s AS relevance_score\n                            FROM {vocabulary_schema}.{vector_table} vectors\n                            INNER JOIN {vocabulary_schema}.concept\n                                ON vectors.concept_id = concept.concept_id\n                            WHERE standard_concept = 'S'\n                                AND concept.concept_class_id NOT IN ({ignore_string_class})\n                                {term_type_clause}\n                            ORDER BY embedding_vector &lt;=&gt; %s\n                            LIMIT {limit * 4} -- May have duplicates due to synonyms\n                        )\n                    ) tmp\n                    GROUP BY concept_id,\n                        concept_name\n                )\n                SELECT target_concept.concept_id,\n                    target_concept.concept_name,\n                    target_concept.relevance_score\n                FROM target_concept\n                ORDER BY relevance_score\n                LIMIT {limit};\n            \"\"\"\n            with self.connection.cursor() as cur:\n                cur.execute(query, (source_vector, source_vector, source_vector, source_vector))\n                results = cur.fetchall()\n        else:\n            query = f\"\"\"\n                WITH target_concept AS (\n                    SELECT concept_id,\n                        concept_name,\n                        MIN(relevance_score) AS relevance_score\n                    FROM (\n                        SELECT concept.concept_id,\n                            concept.concept_name,\n                            embedding_vector &lt;=&gt; %s AS relevance_score\n                        FROM {vocabulary_schema}.{vector_table} vectors\n                        INNER JOIN {vocabulary_schema}.concept\n                            ON vectors.concept_id = concept.concept_id\n                        WHERE standard_concept = 'S'\n                            AND concept.concept_class_id NOT IN ({ignore_string_class})\n                            {term_type_clause}\n                        ORDER BY embedding_vector &lt;=&gt; %s\n                        LIMIT {limit * 4} -- May have duplicates due to synonyms\n                    ) tmp\n                    GROUP BY concept_id,\n                        concept_name\n                )\n                SELECT target_concept.concept_id,\n                    target_concept.concept_name,\n                    target_concept.relevance_score\n                FROM target_concept\n                ORDER BY relevance_score\n                LIMIT {limit};\n            \"\"\"\n            with self.connection.cursor() as cur:\n                cur.execute(query, (source_vector, source_vector))\n                results = cur.fetchall()\n\n        return results\n\n    def search_term(self, term: str, limit: int = 25) -&gt; Optional[pd.DataFrame]:\n        \"\"\"\n        Searches for concepts matching the given term.\n\n        Args:\n            term: The clinical term to search for.\n            limit: The maximum number of results to return.\n\n        Returns:\n            A DataFrame containing the matching concepts, or None if no matches are found.\n        \"\"\"\n        vectors_with_usage = get_embedding_vectors([term])\n        self.cost = self.cost + vectors_with_usage[\"usage\"][\"total_cost_usd\"]\n        vector = vectors_with_usage[\"embeddings\"][0]\n        results = self._search_pgvector(vector, limit)\n        if not results:\n            return None\n        df = pd.DataFrame(\n            results,\n            columns=[\n                \"concept_id\",\n                \"concept_name\",\n                \"score\",\n            ],\n        )\n        return df\n\n    def search_terms(\n        self,\n        df: pd.DataFrame,\n        term_column: str,\n        matched_concept_id_column: str = \"matched_concept_id\",\n        matched_concept_name_column: str = \"matched_concept_name\",\n        match_score_column: str = \"match_score\",\n        match_rank_column: str = \"match_rank\",\n        limit: int = 25,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Searches for concepts matching terms in a DataFrame column.\n\n        Args:\n            df: DataFrame containing the terms to search for.\n            term_column: Name of the column with terms to search.\n            matched_concept_id_column: Name of the column to store matched concept IDs.\n            matched_concept_name_column: Name of the column to store matched concept names.\n            match_score_column: Name of the column to store match scores.\n            match_rank_column: Name of the column to store match ranks.\n            limit: The maximum number of results to return for each term.\n\n        Returns:\n            A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For\n            each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.\n\n        \"\"\"\n\n        vectors_with_usage = get_embedding_vectors(df[term_column].tolist())\n        self.cost = self.cost + vectors_with_usage[\"usage\"][\"total_cost_usd\"]\n        vectors = vectors_with_usage[\"embeddings\"]\n\n        df = df.reset_index(drop=True)\n        all_results = []\n        for index, row in df.iterrows():\n            term = row[term_column]\n            # print(f\"Processing term '{term}'\")\n            vector = vectors[index]\n            results = self._search_pgvector(vector, limit=limit)\n            results = pd.DataFrame(\n                results,\n                columns=[\n                    matched_concept_id_column,\n                    matched_concept_name_column,\n                    match_score_column,\n                ],\n            )\n            results[match_rank_column] = range(1, len(results) + 1)\n            orig_cols = list(df.columns)\n            new_columns = list(results.columns)\n            results[term_column] = term\n            for col in df.columns:\n                results[col] = row[col]\n            results = results[orig_cols + new_columns]\n            all_results.append(results)\n\n        all_results = pd.concat(all_results)\n        return all_results\n\n    def get_total_cost(self) -&gt; float:\n        \"\"\"\n        Returns the total cost incurred for embedding vector calls\n\n        Returns:\n            Total cost in USD.\n        \"\"\"\n\n        return self.cost\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/pgvector_concept_searcher.html#ariadne.vector_search.pgvector_concept_searcher.PgvectorConceptSearcher.get_total_cost","title":"<code>get_total_cost()</code>","text":"<p>Returns the total cost incurred for embedding vector calls</p> <p>Returns:</p> Type Description <code>float</code> <p>Total cost in USD.</p> Source code in <code>src/ariadne/vector_search/pgvector_concept_searcher.py</code> <pre><code>def get_total_cost(self) -&gt; float:\n    \"\"\"\n    Returns the total cost incurred for embedding vector calls\n\n    Returns:\n        Total cost in USD.\n    \"\"\"\n\n    return self.cost\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/pgvector_concept_searcher.html#ariadne.vector_search.pgvector_concept_searcher.PgvectorConceptSearcher.search_term","title":"<code>search_term(term, limit=25)</code>","text":"<p>Searches for concepts matching the given term.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>The clinical term to search for.</p> required <code>limit</code> <code>int</code> <p>The maximum number of results to return.</p> <code>25</code> <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>A DataFrame containing the matching concepts, or None if no matches are found.</p> Source code in <code>src/ariadne/vector_search/pgvector_concept_searcher.py</code> <pre><code>def search_term(self, term: str, limit: int = 25) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Searches for concepts matching the given term.\n\n    Args:\n        term: The clinical term to search for.\n        limit: The maximum number of results to return.\n\n    Returns:\n        A DataFrame containing the matching concepts, or None if no matches are found.\n    \"\"\"\n    vectors_with_usage = get_embedding_vectors([term])\n    self.cost = self.cost + vectors_with_usage[\"usage\"][\"total_cost_usd\"]\n    vector = vectors_with_usage[\"embeddings\"][0]\n    results = self._search_pgvector(vector, limit)\n    if not results:\n        return None\n    df = pd.DataFrame(\n        results,\n        columns=[\n            \"concept_id\",\n            \"concept_name\",\n            \"score\",\n        ],\n    )\n    return df\n</code></pre>"},{"location":"autoapi/ariadne/vector_search/pgvector_concept_searcher.html#ariadne.vector_search.pgvector_concept_searcher.PgvectorConceptSearcher.search_terms","title":"<code>search_terms(df, term_column, matched_concept_id_column='matched_concept_id', matched_concept_name_column='matched_concept_name', match_score_column='match_score', match_rank_column='match_rank', limit=25)</code>","text":"<p>Searches for concepts matching terms in a DataFrame column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the terms to search for.</p> required <code>term_column</code> <code>str</code> <p>Name of the column with terms to search.</p> required <code>matched_concept_id_column</code> <code>str</code> <p>Name of the column to store matched concept IDs.</p> <code>'matched_concept_id'</code> <code>matched_concept_name_column</code> <code>str</code> <p>Name of the column to store matched concept names.</p> <code>'matched_concept_name'</code> <code>match_score_column</code> <code>str</code> <p>Name of the column to store match scores.</p> <code>'match_score'</code> <code>match_rank_column</code> <code>str</code> <p>Name of the column to store match ranks.</p> <code>'match_rank'</code> <code>limit</code> <code>int</code> <p>The maximum number of results to return for each term.</p> <code>25</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For</p> <code>DataFrame</code> <p>each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.</p> Source code in <code>src/ariadne/vector_search/pgvector_concept_searcher.py</code> <pre><code>def search_terms(\n    self,\n    df: pd.DataFrame,\n    term_column: str,\n    matched_concept_id_column: str = \"matched_concept_id\",\n    matched_concept_name_column: str = \"matched_concept_name\",\n    match_score_column: str = \"match_score\",\n    match_rank_column: str = \"match_rank\",\n    limit: int = 25,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Searches for concepts matching terms in a DataFrame column.\n\n    Args:\n        df: DataFrame containing the terms to search for.\n        term_column: Name of the column with terms to search.\n        matched_concept_id_column: Name of the column to store matched concept IDs.\n        matched_concept_name_column: Name of the column to store matched concept names.\n        match_score_column: Name of the column to store match scores.\n        match_rank_column: Name of the column to store match ranks.\n        limit: The maximum number of results to return for each term.\n\n    Returns:\n        A DataFrame containing the same columns as the input dataframe plus the matching concepts for each term. For\n        each term in the input dataframe, multiple rows will be returned corresponding to each matching concept.\n\n    \"\"\"\n\n    vectors_with_usage = get_embedding_vectors(df[term_column].tolist())\n    self.cost = self.cost + vectors_with_usage[\"usage\"][\"total_cost_usd\"]\n    vectors = vectors_with_usage[\"embeddings\"]\n\n    df = df.reset_index(drop=True)\n    all_results = []\n    for index, row in df.iterrows():\n        term = row[term_column]\n        # print(f\"Processing term '{term}'\")\n        vector = vectors[index]\n        results = self._search_pgvector(vector, limit=limit)\n        results = pd.DataFrame(\n            results,\n            columns=[\n                matched_concept_id_column,\n                matched_concept_name_column,\n                match_score_column,\n            ],\n        )\n        results[match_rank_column] = range(1, len(results) + 1)\n        orig_cols = list(df.columns)\n        new_columns = list(results.columns)\n        results[term_column] = term\n        for col in df.columns:\n            results[col] = row[col]\n        results = results[orig_cols + new_columns]\n        all_results.append(results)\n\n    all_results = pd.concat(all_results)\n    return all_results\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/index.html","title":"verbatim_mapping","text":""},{"location":"autoapi/ariadne/verbatim_mapping/term_downloader.html","title":"term_downloader","text":""},{"location":"autoapi/ariadne/verbatim_mapping/term_downloader.html#ariadne.verbatim_mapping.term_downloader.download_terms","title":"<code>download_terms(config=Config())</code>","text":"<p>Download terms from vocabulary database and store them in parquet files for use in verbatim mapping.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>A Config object containing configuration parameters. This function uses the verbatim_mapping section of the config, which specifies the vocabularies, domains, etc. to filter the terms to be downloaded.</p> <code>Config()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/ariadne/verbatim_mapping/term_downloader.py</code> <pre><code>def download_terms(config: Config = Config()) -&gt; None:\n    \"\"\"\n    Download terms from vocabulary database and store them in parquet files for use in verbatim mapping.\n\n    Args:\n        config: A Config object containing configuration parameters. This function uses the verbatim_mapping section of\n            the config, which specifies the vocabularies, domains, etc. to filter the terms to be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    # Check if Parquet files already exist. Skip download if they do.\n    if os.path.exists(config.system.terms_folder) and os.listdir(config.system.terms_folder):\n        print(f\"Parquet files already exist in folder {config.system.terms_folder}. Skipping download.\")\n        return\n\n    os.makedirs(config.system.log_folder, exist_ok=True)\n    os.makedirs(config.system.terms_folder, exist_ok=True)\n    open_log(os.path.join(config.system.log_folder, \"logDownloadTerms.txt\"))\n\n    logging.info(\"Starting downloading terms\")\n\n    engine = create_engine(get_environment_variable(\"VOCAB_CONNECTION_STRING\"))\n    query = _create_query(engine=engine, config=config)\n\n    with engine.connect() as connection:\n        terms_result_set = connection.execution_options(stream_results=True).execute(query)\n        total_inserted = 0\n        while True:\n            chunk = terms_result_set.fetchmany(config.system.download_batch_size)\n            if not chunk:\n                break\n            _store_in_parquet(\n                concept_ids=[row.concept_id for row in chunk],\n                terms=[row.term for row in chunk],\n                concept_names=[row.concept_name for row in chunk],\n                # vocabulary_ids=[row.vocabulary_id for row in chunk],\n                # domain_ids=[row.domain_id for row in chunk],\n                # standard_concepts=[row.standard_concept for row in chunk],\n                # sources=[row.source for row in chunk],\n                file_name=os.path.join(\n                    config.system.terms_folder,\n                    f\"Terms_{total_inserted + 1}_{total_inserted + len(chunk)}.parquet\",\n                ),\n            )\n            total_inserted += len(chunk)\n            logging.info(f\"Downloaded {len(chunk)} rows, total downloaded: {total_inserted}\")\n    logging.info(\"Finished downloading terms\")\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer.html","title":"term_normalizer","text":""},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer.html#ariadne.verbatim_mapping.term_normalizer.TermNormalizer","title":"<code>TermNormalizer</code>","text":"<p>Normalizes clinical term strings for high-precision matching.</p> Source code in <code>src/ariadne/verbatim_mapping/term_normalizer.py</code> <pre><code>class TermNormalizer:\n    \"\"\"\n    Normalizes clinical term strings for high-precision matching.\n    \"\"\"\n\n    def __init__(self, config: Config = Config()):\n        self.config = config\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n            print(\"spaCy model 'en_core_web_sm' loaded successfully.\")\n        except IOError:\n            print(\"spaCy model 'en_core_web_sm' not found.\")\n            print(\"Please run: python -m spacy download en_core_web_sm\")\n            raise\n\n    def normalize_term(self, term: str) -&gt; str:\n        \"\"\"\n        Normalizes a clinical term string for high-precision matching.\n\n        The pipeline is:\n\n        1. Convert to lowercase.\n        2. Remove possessive \"'s\" at the end of words.\n        3. Remove specific non-informative substrings (e.g., '(disorder)'). The strings are taken from the\n           substrings_to_remove list in the config yaml file\n        4. Remove all punctuation.\n        5. Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").\n        6. Join tokens into a single string, preserving order.\n\n        This makes \"liver disorders\" and \"Liver-Disorders (disorder)\"\n        both normalize to \"liver disorder\".\n\n        Args:\n            term: The clinical term string to normalize.\n        Returns:\n            The normalized term string.\n        \"\"\"\n        # 1. Convert to lowercase\n        term = term.lower()\n\n        # 2. Remove possessive 's at the end of a word\n        # This handles \"Alzheimer's disease\" -&gt; \"Alzheimer disease\"\n        # It finds a word character (\\w) followed by 's and a word boundary (\\b),\n        # and replaces the whole thing with just the captured word character (group 1).\n        term = re.sub(r\"(\\w)'s\\b\", r\"\\1\", term)\n\n        # 3. Remove specific non-informative substrings\n        for sub in self.config.verbatim_mapping.substrings_to_remove:\n            term = term.replace(sub, ' ')\n\n        # 4. Remove all punctuation (replace with a space)\n        # This handles \"liver-disorder\" and \"liver, disorder\"\n        term = re.sub(r'[^\\w\\s]', ' ', term)\n\n        # 5. Tokenize and lemmatize using spaCy\n        doc = self.nlp(term)\n\n        processed_tokens = []\n        for token in doc:\n            # Get the lemma (base form)\n            lemma = token.lemma_\n\n            # 6. Remove empty tokens (from extra spaces)\n            if lemma.strip():\n                processed_tokens.append(lemma)\n\n        # 7. Join tokens into a single string\n        return \" \".join(processed_tokens)\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer.html#ariadne.verbatim_mapping.term_normalizer.TermNormalizer.normalize_term","title":"<code>normalize_term(term)</code>","text":"<p>Normalizes a clinical term string for high-precision matching.</p> <p>The pipeline is:</p> <ol> <li>Convert to lowercase.</li> <li>Remove possessive \"'s\" at the end of words.</li> <li>Remove specific non-informative substrings (e.g., '(disorder)'). The strings are taken from the    substrings_to_remove list in the config yaml file</li> <li>Remove all punctuation.</li> <li>Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").</li> <li>Join tokens into a single string, preserving order.</li> </ol> <p>This makes \"liver disorders\" and \"Liver-Disorders (disorder)\" both normalize to \"liver disorder\".</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>The clinical term string to normalize.</p> required <p>Returns:     The normalized term string.</p> Source code in <code>src/ariadne/verbatim_mapping/term_normalizer.py</code> <pre><code>def normalize_term(self, term: str) -&gt; str:\n    \"\"\"\n    Normalizes a clinical term string for high-precision matching.\n\n    The pipeline is:\n\n    1. Convert to lowercase.\n    2. Remove possessive \"'s\" at the end of words.\n    3. Remove specific non-informative substrings (e.g., '(disorder)'). The strings are taken from the\n       substrings_to_remove list in the config yaml file\n    4. Remove all punctuation.\n    5. Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").\n    6. Join tokens into a single string, preserving order.\n\n    This makes \"liver disorders\" and \"Liver-Disorders (disorder)\"\n    both normalize to \"liver disorder\".\n\n    Args:\n        term: The clinical term string to normalize.\n    Returns:\n        The normalized term string.\n    \"\"\"\n    # 1. Convert to lowercase\n    term = term.lower()\n\n    # 2. Remove possessive 's at the end of a word\n    # This handles \"Alzheimer's disease\" -&gt; \"Alzheimer disease\"\n    # It finds a word character (\\w) followed by 's and a word boundary (\\b),\n    # and replaces the whole thing with just the captured word character (group 1).\n    term = re.sub(r\"(\\w)'s\\b\", r\"\\1\", term)\n\n    # 3. Remove specific non-informative substrings\n    for sub in self.config.verbatim_mapping.substrings_to_remove:\n        term = term.replace(sub, ' ')\n\n    # 4. Remove all punctuation (replace with a space)\n    # This handles \"liver-disorder\" and \"liver, disorder\"\n    term = re.sub(r'[^\\w\\s]', ' ', term)\n\n    # 5. Tokenize and lemmatize using spaCy\n    doc = self.nlp(term)\n\n    processed_tokens = []\n    for token in doc:\n        # Get the lemma (base form)\n        lemma = token.lemma_\n\n        # 6. Remove empty tokens (from extra spaces)\n        if lemma.strip():\n            processed_tokens.append(lemma)\n\n    # 7. Join tokens into a single string\n    return \" \".join(processed_tokens)\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper.html","title":"verbatim_term_mapper","text":""},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper.html#ariadne.verbatim_mapping.verbatim_term_mapper.VerbatimTermMapper","title":"<code>VerbatimTermMapper</code>","text":"<p>Maps a source term to a provided subset of target concepts based on exact matches of normalized terms.</p> Source code in <code>src/ariadne/verbatim_mapping/verbatim_term_mapper.py</code> <pre><code>class VerbatimTermMapper:\n    \"\"\"\n    Maps a source term to a provided subset of target concepts based on exact matches of normalized terms.\n    \"\"\"\n\n    def __init__(self, config: Config = Config()):\n        self.term_normalizer = TermNormalizer(config)\n\n    def map_term(\n        self,\n        source_term: str,\n        target_concept_ids: List[int],\n        target_terms: List[str],\n        target_synonyms: List[str],\n    ) -&gt; (Union[int, None], Union[str, None]):\n        \"\"\"\n        Maps a source term to the best matching target concept ID based on normalized terms.\n\n        Args:\n            source_term: the source clinical term to map\n            target_concept_ids: a list of target concept IDs\n            target_terms: a list of target clinical terms\n            target_synonyms: a list of target synonyms. Each string is semicolon separated synonyms for the\n                corresponding target term.\n\n        Returns:\n            A tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)\n        \"\"\"\n        normalized_source = self.term_normalizer.normalize_term(source_term)\n        for concept_id, term, synonyms in zip(\n            target_concept_ids, target_terms, target_synonyms\n        ):\n            normalized_term = self.term_normalizer.normalize_term(term)\n            if normalized_source == normalized_term:\n                return concept_id, term\n            if not pd.isna(synonyms):\n                for synonym in synonyms.split(\";\"):\n                    normalized_synonym = self.term_normalizer.normalize_term(synonym)\n                    if normalized_source == normalized_synonym:\n                        return concept_id, term\n        return None, None\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper.html#ariadne.verbatim_mapping.verbatim_term_mapper.VerbatimTermMapper.map_term","title":"<code>map_term(source_term, target_concept_ids, target_terms, target_synonyms)</code>","text":"<p>Maps a source term to the best matching target concept ID based on normalized terms.</p> <p>Parameters:</p> Name Type Description Default <code>source_term</code> <code>str</code> <p>the source clinical term to map</p> required <code>target_concept_ids</code> <code>List[int]</code> <p>a list of target concept IDs</p> required <code>target_terms</code> <code>List[str]</code> <p>a list of target clinical terms</p> required <code>target_synonyms</code> <code>List[str]</code> <p>a list of target synonyms. Each string is semicolon separated synonyms for the corresponding target term.</p> required <p>Returns:</p> Type Description <code>(Union[int, None], Union[str, None])</code> <p>A tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)</p> Source code in <code>src/ariadne/verbatim_mapping/verbatim_term_mapper.py</code> <pre><code>def map_term(\n    self,\n    source_term: str,\n    target_concept_ids: List[int],\n    target_terms: List[str],\n    target_synonyms: List[str],\n) -&gt; (Union[int, None], Union[str, None]):\n    \"\"\"\n    Maps a source term to the best matching target concept ID based on normalized terms.\n\n    Args:\n        source_term: the source clinical term to map\n        target_concept_ids: a list of target concept IDs\n        target_terms: a list of target clinical terms\n        target_synonyms: a list of target synonyms. Each string is semicolon separated synonyms for the\n            corresponding target term.\n\n    Returns:\n        A tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)\n    \"\"\"\n    normalized_source = self.term_normalizer.normalize_term(source_term)\n    for concept_id, term, synonyms in zip(\n        target_concept_ids, target_terms, target_synonyms\n    ):\n        normalized_term = self.term_normalizer.normalize_term(term)\n        if normalized_source == normalized_term:\n            return concept_id, term\n        if not pd.isna(synonyms):\n            for synonym in synonyms.split(\";\"):\n                normalized_synonym = self.term_normalizer.normalize_term(synonym)\n                if normalized_source == normalized_synonym:\n                    return concept_id, term\n    return None, None\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.html","title":"vocab_verbatim_term_mapper","text":""},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.html#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.VocabVerbatimTermMapper","title":"<code>VocabVerbatimTermMapper</code>","text":"<p>Maps source terms to concept IDs using a pre-built index of normalized terms. The index is created from vocabulary term files stored in Parquet format, downloaded using the download_terms module.</p> <ol> <li>If an index file exists at the verbatim_mapping_index_file path specified in the config, it is loaded.</li> <li>If not, the index is created by processing all Parquet files in the terms folder specified in the config.</li> </ol> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>class VocabVerbatimTermMapper:\n    \"\"\"\n    Maps source terms to concept IDs using a pre-built index of normalized terms.\n    The index is created from vocabulary term files stored in Parquet format, downloaded using the download_terms\n    module.\n\n    1. If an index file exists at the verbatim_mapping_index_file path specified in the config, it is loaded.\n    2. If not, the index is created by processing all Parquet files in the terms folder specified in the config.\n    \"\"\"\n\n    def __init__(self, config: Config = Config()):\n        self.term_normalizer = TermNormalizer(config)\n        if os.path.exists(config.system.verbatim_mapping_index_file):\n            with open(config.system.verbatim_mapping_index_file, \"rb\") as handle:\n                self.index = pickle.load(handle)\n            print(f\"Index loaded from {config.system.verbatim_mapping_index_file}\")\n        else:\n            self._create_index(config)\n\n    def _create_index(self, config: Config):\n        print(\"Creating index\")\n        if not os.path.exists(config.system.terms_folder):\n            raise FileNotFoundError(\n                f\"Terms folder {config.system.terms_folder} does not exist. Make sure to run the download_terms module first.\"\n            )\n        all_files = [\n            os.path.join(config.system.terms_folder, f)\n            for f in os.listdir(config.system.terms_folder)\n            if f.endswith(\".parquet\")\n        ]\n        pool = multiprocessing.get_context(\"spawn\").Pool(processes=config.system.max_cores)\n        index_data = {}\n        for file in all_files:\n            print(f\"Processing file: {file}\")\n            df = pd.read_parquet(file)\n            normalized_terms = pool.map(self.term_normalizer.normalize_term, df[\"term\"].tolist())\n            for norm_term, concept_id, concept_name in zip(\n                normalized_terms, df[\"concept_id\"].tolist(), df[\"concept_name\"].tolist()\n            ):\n                concept = (int(concept_id), concept_name)\n                if norm_term in index_data:\n                    existing = index_data[norm_term]\n                    if isinstance(existing, list):\n                        if concept_id not in [c[0] for c in existing]:\n                            existing.append(concept)\n                    else:\n                        if concept_id != existing[0]:\n                            index_data[norm_term] = [existing, concept]\n                else:\n                    index_data[norm_term] = concept\n\n        pool.close()\n        self.index = index_data\n\n        try:\n            with open(config.system.verbatim_mapping_index_file, \"wb\") as f:\n                pickle.dump(index_data, f)\n            print(f\"Index saved to {config.system.verbatim_mapping_index_file}\")\n        except OSError as e:\n            print(f\"Error saving index: {e}\")\n\n    def map_term(self, source_term: str) -&gt; List[tuple[int, str]]:\n        \"\"\"\n        Maps a source term to concept IDs using the pre-built index.\n\n        Args:\n            source_term: the source clinical term to map\n\n        Returns:\n            A list of concept ID - concept name tuples, possibly empty if no match is found.\n        \"\"\"\n        normalized_source = self.term_normalizer.normalize_term(source_term)\n        if normalized_source in self.index:\n            concepts = self.index[normalized_source]\n            if isinstance(concepts, list):\n                return concepts\n            else:\n                return [concepts]\n        return []\n\n    def map_terms(\n        self,\n        source_terms: pd.DataFrame,\n        term_column: str = \"cleaned_term\",\n        mapped_concept_id_column: str = \"mapped_concept_id\",\n        mapped_concept_name_column: str = \"mapped_concept_name\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Maps source terms in a DataFrame column to concept IDs using the pre-built index.\n\n        Args:\n            source_terms: DataFrame containing the source clinical terms to map\n            term_column: Name of the column with terms to map\n            mapped_concept_id_column: Name of the column to store matched concept IDs.\n            mapped_concept_name_column: Name of the column to store matched concept names.\n\n        Returns:\n            A DataFrame with the original columns and their mapped concept IDs and names.\n        \"\"\"\n        source_terms[[mapped_concept_id_column, mapped_concept_name_column]] = source_terms[term_column].apply(\n            lambda term: pd.Series(self.map_term(term)[0] if self.map_term(term) else (-1, \"\"))\n        )\n        return source_terms\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.html#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.VocabVerbatimTermMapper.map_term","title":"<code>map_term(source_term)</code>","text":"<p>Maps a source term to concept IDs using the pre-built index.</p> <p>Parameters:</p> Name Type Description Default <code>source_term</code> <code>str</code> <p>the source clinical term to map</p> required <p>Returns:</p> Type Description <code>List[tuple[int, str]]</code> <p>A list of concept ID - concept name tuples, possibly empty if no match is found.</p> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>def map_term(self, source_term: str) -&gt; List[tuple[int, str]]:\n    \"\"\"\n    Maps a source term to concept IDs using the pre-built index.\n\n    Args:\n        source_term: the source clinical term to map\n\n    Returns:\n        A list of concept ID - concept name tuples, possibly empty if no match is found.\n    \"\"\"\n    normalized_source = self.term_normalizer.normalize_term(source_term)\n    if normalized_source in self.index:\n        concepts = self.index[normalized_source]\n        if isinstance(concepts, list):\n            return concepts\n        else:\n            return [concepts]\n    return []\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.html#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.VocabVerbatimTermMapper.map_terms","title":"<code>map_terms(source_terms, term_column='cleaned_term', mapped_concept_id_column='mapped_concept_id', mapped_concept_name_column='mapped_concept_name')</code>","text":"<p>Maps source terms in a DataFrame column to concept IDs using the pre-built index.</p> <p>Parameters:</p> Name Type Description Default <code>source_terms</code> <code>DataFrame</code> <p>DataFrame containing the source clinical terms to map</p> required <code>term_column</code> <code>str</code> <p>Name of the column with terms to map</p> <code>'cleaned_term'</code> <code>mapped_concept_id_column</code> <code>str</code> <p>Name of the column to store matched concept IDs.</p> <code>'mapped_concept_id'</code> <code>mapped_concept_name_column</code> <code>str</code> <p>Name of the column to store matched concept names.</p> <code>'mapped_concept_name'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the original columns and their mapped concept IDs and names.</p> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>def map_terms(\n    self,\n    source_terms: pd.DataFrame,\n    term_column: str = \"cleaned_term\",\n    mapped_concept_id_column: str = \"mapped_concept_id\",\n    mapped_concept_name_column: str = \"mapped_concept_name\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Maps source terms in a DataFrame column to concept IDs using the pre-built index.\n\n    Args:\n        source_terms: DataFrame containing the source clinical terms to map\n        term_column: Name of the column with terms to map\n        mapped_concept_id_column: Name of the column to store matched concept IDs.\n        mapped_concept_name_column: Name of the column to store matched concept names.\n\n    Returns:\n        A DataFrame with the original columns and their mapped concept IDs and names.\n    \"\"\"\n    source_terms[[mapped_concept_id_column, mapped_concept_name_column]] = source_terms[term_column].apply(\n        lambda term: pd.Series(self.map_term(term)[0] if self.map_term(term) else (-1, \"\"))\n    )\n    return source_terms\n</code></pre>"},{"location":"notebooks/01_evaluate_exact_mapping.html","title":"Evaluation of the exact matching pipeline","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\nimport pandas as pd\n\nproject_root = Path.cwd().parent\n</pre> from pathlib import Path import pandas as pd  project_root = Path.cwd().parent In\u00a0[2]: Copied! <pre>gold_standard_path = project_root / \"data\" / \"gold_standards\" / \"exact_matching_gs.csv\"\n\ngold_standard = pd.read_csv(gold_standard_path)\ngold_standard.head()\n</pre> gold_standard_path = project_root / \"data\" / \"gold_standards\" / \"exact_matching_gs.csv\"  gold_standard = pd.read_csv(gold_standard_path) gold_standard.head() Out[2]: source_concept_id source_term target_concept_id target_concept_name predicate target_concept_id_b target_concept_name_b predicate_b 0 8690 Unspecified enthesopathy, lower limb, excludin... 4116324 Enthesopathy of lower leg and ankle region exactMatch 4116324.0 Enthesopathy of lower leg and ankle region broadMatch 1 9724 Lead-induced chronic gout, unspecified elbow 607432 Chronic gout caused by lead broadMatch NaN NaN NaN 2 9770 Chronic gout due to renal impairment, right elbow 46270464 Gout of elbow due to renal impairment broadMatch NaN NaN NaN 3 9946 Pathological fracture, left ankle 760649 Pathological fracture of left ankle exactMatch NaN NaN NaN 4 10389 Unspecified fracture of skull 4324690 Fracture of skull exactMatch NaN NaN NaN <p>This gold standard contains the following columns:</p> <ul> <li><code>source_concept_id</code>: the concept ID of the source term (non-standard concept)</li> <li><code>source_term</code>: the source term text</li> <li><code>standard_concept_id</code>: the concept ID of the standard concept to which the source term maps</li> <li><code>standard_concept_name</code>: the name of the standard concept</li> <li><code>predicate</code>: Whether the mapping is an <code>exactMatch</code> or a <code>broadMatch</code>. Only <code>exactMatch</code> mappings are considered correct in this evaluation.</li> <li><code>target_concept_id_b</code>, <code>target_concept_name_b</code>, and <code>predicate_b</code>: optional second mapping for the source term that is equally valid, if available.</li> </ul> In\u00a0[3]: Copied! <pre>from ariadne.term_cleanup.term_cleaner import TermCleaner\n\ncleaned_terms_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_cleaned_terms.csv\"\nif cleaned_terms_file.exists():\n    cleaned_terms = pd.read_csv(cleaned_terms_file)\n    print(\"Loaded cleaned terms from file.\")\nelse:\n    term_cleaner = TermCleaner()\n    cleaned_terms = term_cleaner.clean_terms(gold_standard)\n    print(f\"Total LLM cost: ${term_cleaner.get_total_cost():.6f}\")\ncleaned_terms.to_csv(cleaned_terms_file, index=False)\n\n\ncleaned_terms[[\"source_term\", \"cleaned_term\"]].head(10)\n</pre> from ariadne.term_cleanup.term_cleaner import TermCleaner  cleaned_terms_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_cleaned_terms.csv\" if cleaned_terms_file.exists():     cleaned_terms = pd.read_csv(cleaned_terms_file)     print(\"Loaded cleaned terms from file.\") else:     term_cleaner = TermCleaner()     cleaned_terms = term_cleaner.clean_terms(gold_standard)     print(f\"Total LLM cost: ${term_cleaner.get_total_cost():.6f}\") cleaned_terms.to_csv(cleaned_terms_file, index=False)   cleaned_terms[[\"source_term\", \"cleaned_term\"]].head(10) <pre>Loaded cleaned terms from file.\n</pre> Out[3]: source_term cleaned_term 0 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 1 Lead-induced chronic gout, unspecified elbow Lead-induced chronic gout, elbow 2 Chronic gout due to renal impairment, right elbow Chronic gout due to renal impairment, right elbow 3 Pathological fracture, left ankle Pathological fracture, left ankle 4 Unspecified fracture of skull fracture of skull 5 Ocular laceration without prolapse or loss of ... Ocular laceration without prolapse or loss of ... 6 Penetrating wound of orbit with or without for... Penetrating wound of orbit 7 Fracture of unspecified shoulder girdle, part ... Fracture of shoulder girdle 8 Drowning and submersion due to falling or jump... Drowning and submersion 9 Alcohol use, unspecified with withdrawal delirium Alcohol use with withdrawal delirium In\u00a0[4]: Copied! <pre>from ariadne.verbatim_mapping.term_downloader import download_terms\nfrom ariadne.verbatim_mapping.vocab_verbatim_term_mapper import VocabVerbatimTermMapper\n\nverbatim_match_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_verbatim_maps.csv\"\nif verbatim_match_file.exists():\n    verbatim_matches = pd.read_csv(verbatim_match_file)\n    print(\"Loaded verbatim matches from file.\")\nelse:\n    download_terms() # Downloads the terms as Parquet files to the folder specified in config.yaml.\n    verbatim_mapper = VocabVerbatimTermMapper() # Will construct the vocabulary index if needed\n    verbatim_matches = verbatim_mapper.map_terms(cleaned_terms)\n    verbatim_matches.to_csv(verbatim_match_file, index=False)\nverbatim_matches[[\"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"]].head(10)\n</pre> from ariadne.verbatim_mapping.term_downloader import download_terms from ariadne.verbatim_mapping.vocab_verbatim_term_mapper import VocabVerbatimTermMapper  verbatim_match_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_verbatim_maps.csv\" if verbatim_match_file.exists():     verbatim_matches = pd.read_csv(verbatim_match_file)     print(\"Loaded verbatim matches from file.\") else:     download_terms() # Downloads the terms as Parquet files to the folder specified in config.yaml.     verbatim_mapper = VocabVerbatimTermMapper() # Will construct the vocabulary index if needed     verbatim_matches = verbatim_mapper.map_terms(cleaned_terms)     verbatim_matches.to_csv(verbatim_match_file, index=False) verbatim_matches[[\"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"]].head(10) <pre>Loaded verbatim matches from file.\n</pre> Out[4]: source_term cleaned_term mapped_concept_id mapped_concept_name 0 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot -1 NaN 1 Lead-induced chronic gout, unspecified elbow Lead-induced chronic gout, elbow -1 NaN 2 Chronic gout due to renal impairment, right elbow Chronic gout due to renal impairment, right elbow -1 NaN 3 Pathological fracture, left ankle Pathological fracture, left ankle -1 NaN 4 Unspecified fracture of skull fracture of skull 4324690 Fracture of skull 5 Ocular laceration without prolapse or loss of ... Ocular laceration without prolapse or loss of ... -1 NaN 6 Penetrating wound of orbit with or without for... Penetrating wound of orbit 4334734 Penetrating wound of orbit 7 Fracture of unspecified shoulder girdle, part ... Fracture of shoulder girdle -1 NaN 8 Drowning and submersion due to falling or jump... Drowning and submersion -1 NaN 9 Alcohol use, unspecified with withdrawal delirium Alcohol use with withdrawal delirium -1 NaN In\u00a0[5]: Copied! <pre>from ariadne.vector_search.pgvector_concept_searcher import PgvectorConceptSearcher\n# from ariadne.vector_search.hecate_concept_searcher import HecateConceptSearcher\n\nvector_search_results_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_results.csv\"\nif vector_search_results_file.exists():\n    vector_search_results = pd.read_csv(vector_search_results_file)\n    print(\"Loaded vector search results from file.\")\nelse:\n    concept_searcher = PgvectorConceptSearcher()\n    # concept_searcher = HecateConceptSearcher()\n    unmatched_terms = cleaned_terms.copy()\n    unmatched_terms = unmatched_terms[unmatched_terms[\"source_concept_id\"].isin(\n        verbatim_matches[\"source_concept_id\"][verbatim_matches[\"mapped_concept_id\"] == -1]\n    )]\n    vector_search_results = concept_searcher.search_terms(\n        unmatched_terms, term_column=\"cleaned_term\"\n    )\n    vector_search_results.to_csv(vector_search_results_file, index=False)\nvector_search_results[[\"source_term\", \"cleaned_term\", \"matched_concept_id\", \"matched_concept_name\", \"match_score\"]].head(10)\n</pre> from ariadne.vector_search.pgvector_concept_searcher import PgvectorConceptSearcher # from ariadne.vector_search.hecate_concept_searcher import HecateConceptSearcher  vector_search_results_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_results.csv\" if vector_search_results_file.exists():     vector_search_results = pd.read_csv(vector_search_results_file)     print(\"Loaded vector search results from file.\") else:     concept_searcher = PgvectorConceptSearcher()     # concept_searcher = HecateConceptSearcher()     unmatched_terms = cleaned_terms.copy()     unmatched_terms = unmatched_terms[unmatched_terms[\"source_concept_id\"].isin(         verbatim_matches[\"source_concept_id\"][verbatim_matches[\"mapped_concept_id\"] == -1]     )]     vector_search_results = concept_searcher.search_terms(         unmatched_terms, term_column=\"cleaned_term\"     )     vector_search_results.to_csv(vector_search_results_file, index=False) vector_search_results[[\"source_term\", \"cleaned_term\", \"matched_concept_id\", \"matched_concept_name\", \"match_score\"]].head(10) <pre>Loaded vector search results from file.\n</pre> Out[5]: source_term cleaned_term matched_concept_id matched_concept_name match_score 0 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 4194889 Enthesopathy of lower limb 0.071184 1 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 46284922 Enthesitis of lower limb 0.233134 2 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 198846 Enthesopathy of hip region 0.237688 3 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 4347178 Enthesopathy of foot region 0.264289 4 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 73008 Enthesopathy 0.267237 5 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 4116324 Enthesopathy of lower leg and ankle region 0.270949 6 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 37108924 Enthesopathy of left foot 0.285147 7 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 37309727 Bilateral enthesopathy of feet 0.307093 8 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 37108923 Enthesopathy of right foot 0.308362 9 Unspecified enthesopathy, lower limb, excludin... enthesopathy, lower limb, excluding foot 42537682 Enthesopathy of upper limb 0.322310 In\u00a0[6]: Copied! <pre>from ariadne.evaluation.concept_search_evaluator import evaluate_concept_search\n\nevaluate_concept_search(\n    search_results=vector_search_results,\n    output_file=project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_evaluation.txt\",\n)\n\nwith open(project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_evaluation.txt\", \"r\") as f:\n    evaluation_text = f.read()\nprint(\"\\n\".join(evaluation_text.split(\"\\n\")[:15]))\n</pre> from ariadne.evaluation.concept_search_evaluator import evaluate_concept_search  evaluate_concept_search(     search_results=vector_search_results,     output_file=project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_evaluation.txt\", )  with open(project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_evaluation.txt\", \"r\") as f:     evaluation_text = f.read() print(\"\\n\".join(evaluation_text.split(\"\\n\")[:15]))  <pre>Evaluation complete. Results written to E:\\git\\Ariadne\\data\\notebook_results\\exact_matching_vector_search_evaluation.txt\nEvaluated gold standard concepts: 333\nMean Average Precision: 0.8651153955075523\nRecall@1: 0.7957957957957958\nRecall@3: 0.924924924924925\nRecall@10: 0.978978978978979\nRecall@25: 0.993993993993994\n\nSource term: Unspecified enthesopathy, lower limb, excluding foot (8690)\nSearched term: enthesopathy, lower limb, excluding foot\nGold standard concept rank: 6\n\n match_rank Correct  matched_concept_id                       matched_concept_name\n          1                     4194889                 Enthesopathy of lower limb\n          2                    46284922                   Enthesitis of lower limb\n          3                      198846                 Enthesopathy of hip region\n</pre> In\u00a0[7]: Copied! <pre>from ariadne.llm_mapping.concept_context_retriever import add_concept_context\n\ncontext_file_name = project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_context.csv\"\nif context_file_name.exists():\n    vector_search_results = pd.read_csv(context_file_name)\n    print(\"Loaded vector search context from file.\")\nelse:\n    vector_search_results = add_concept_context(vector_search_results)\n    vector_search_results.to_csv(context_file_name, index=False)\nvector_search_results[[\"source_term\", \"matched_concept_id\", \"matched_concept_name\", \"matched_synonyms\", \"matched_concept_class_id\", \"matched_children\"]].head(10)\n</pre> from ariadne.llm_mapping.concept_context_retriever import add_concept_context  context_file_name = project_root / \"data\" / \"notebook_results\" / \"exact_matching_vector_search_context.csv\" if context_file_name.exists():     vector_search_results = pd.read_csv(context_file_name)     print(\"Loaded vector search context from file.\") else:     vector_search_results = add_concept_context(vector_search_results)     vector_search_results.to_csv(context_file_name, index=False) vector_search_results[[\"source_term\", \"matched_concept_id\", \"matched_concept_name\", \"matched_synonyms\", \"matched_concept_class_id\", \"matched_children\"]].head(10) Out[7]: source_term matched_concept_id matched_concept_name matched_synonyms matched_concept_class_id matched_children 0 Unspecified enthesopathy, lower limb, excludin... 4194889 Enthesopathy of lower limb Enthesopathy of lower limb (disorder) Disorder Enthesopathy of hip region;Enthesopathy of ank... 1 Unspecified enthesopathy, lower limb, excludin... 46284922 Enthesitis of lower limb Enthesitis of lower limb (disorder) Disorder Enthesitis of knee;Enthesitis of quadriceps te... 2 Unspecified enthesopathy, lower limb, excludin... 198846 Enthesopathy of hip region Enthesopathy of hip region (disorder) Disorder Enthesopathy of left hip;Enthesopathy of right... 3 Unspecified enthesopathy, lower limb, excludin... 4347178 Enthesopathy of foot region Enthesopathy of foot region (disorder) Disorder Tarsus enthesopathy;Enthesopathy of right foot... 4 Unspecified enthesopathy, lower limb, excludin... 73008 Enthesopathy Enthesopathy (disorder) Disorder Peripheral enthesopathy;Enthesitis;Spinal enth... 5 Unspecified enthesopathy, lower limb, excludin... 4116324 Enthesopathy of lower leg and ankle region Enthesopathy of lower leg and ankle region (di... Disorder None 6 Unspecified enthesopathy, lower limb, excludin... 37108924 Enthesopathy of left foot Enthesopathy of left foot (disorder) Disorder Bilateral enthesopathy of feet;Enthesopathy of... 7 Unspecified enthesopathy, lower limb, excludin... 37309727 Bilateral enthesopathy of feet Enthesopathy of bilateral feet (disorder);Enth... Disorder None 8 Unspecified enthesopathy, lower limb, excludin... 37108923 Enthesopathy of right foot Enthesopathy of right foot (disorder) Disorder Bilateral enthesopathy of feet;Enthesopathy of... 9 Unspecified enthesopathy, lower limb, excludin... 42537682 Enthesopathy of upper limb Enthesopathy of upper limb (disorder) Disorder Enthesopathy of elbow region;Enthesopathy of s... In\u00a0[\u00a0]: Copied! <pre>from ariadne.llm_mapping.llm_mapper import LlmMapper\n\nllm_mapper = LlmMapper()\nmapped_terms = llm_mapper.map_terms(vector_search_results)\n\nllm_mapped_terms_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_llm_mapped_terms.csv\"\nmapped_terms.to_csv(llm_mapped_terms_file, index=False)\nprint(f\"Total LLM cost: ${llm_mapper.get_total_cost():.6f}\")\nmapped_terms[[\"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"]].head(10)\n</pre> from ariadne.llm_mapping.llm_mapper import LlmMapper  llm_mapper = LlmMapper() mapped_terms = llm_mapper.map_terms(vector_search_results)  llm_mapped_terms_file = project_root / \"data\" / \"notebook_results\" / \"exact_matching_llm_mapped_terms.csv\" mapped_terms.to_csv(llm_mapped_terms_file, index=False) print(f\"Total LLM cost: ${llm_mapper.get_total_cost():.6f}\") mapped_terms[[\"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"]].head(10) In\u00a0[\u00a0]: Copied! <pre>from ariadne.evaluation.concept_selection_evaluator import evaluate\n\n# Combine verbatim matches and LLM matches\n# First take all verbatim matches that were successful\nfinal_mapped_terms = verbatim_matches[verbatim_matches[\"mapped_concept_id\"] != -1][\n    [\"source_concept_id\", \"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"]\n].copy()\nfinal_mapped_terms[\"map_method\"] = \"verbatim\"\n\n# Then add the LLM matches (for the terms that were not verbatim matched)\nllm_mapped_terms_filtered = mapped_terms[\n    [\"source_concept_id\", \"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"]\n].copy()\nllm_mapped_terms_filtered[\"map_method\"] = \"llm\"\nfinal_mapped_terms = pd.concat([final_mapped_terms, llm_mapped_terms_filtered], ignore_index=True)\n\n# Evaluate\nfinal_evaluation_results = evaluate(final_mapped_terms)\nfinal_evaluation_results.to_csv(project_root / \"data\" / \"notebook_results\" / \"exact_matching_final_evaluation.csv\", index=False)\n\nfinal_evaluation_results[[\"source_term\", \"target_concept_name\", \"map_method\", \"mapped_concept_name\", \"is_correct\"]].head()\n</pre> from ariadne.evaluation.concept_selection_evaluator import evaluate  # Combine verbatim matches and LLM matches # First take all verbatim matches that were successful final_mapped_terms = verbatim_matches[verbatim_matches[\"mapped_concept_id\"] != -1][     [\"source_concept_id\", \"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"] ].copy() final_mapped_terms[\"map_method\"] = \"verbatim\"  # Then add the LLM matches (for the terms that were not verbatim matched) llm_mapped_terms_filtered = mapped_terms[     [\"source_concept_id\", \"source_term\", \"cleaned_term\", \"mapped_concept_id\", \"mapped_concept_name\"] ].copy() llm_mapped_terms_filtered[\"map_method\"] = \"llm\" final_mapped_terms = pd.concat([final_mapped_terms, llm_mapped_terms_filtered], ignore_index=True)  # Evaluate final_evaluation_results = evaluate(final_mapped_terms) final_evaluation_results.to_csv(project_root / \"data\" / \"notebook_results\" / \"exact_matching_final_evaluation.csv\", index=False)  final_evaluation_results[[\"source_term\", \"target_concept_name\", \"map_method\", \"mapped_concept_name\", \"is_correct\"]].head()"},{"location":"notebooks/01_evaluate_exact_mapping.html#evaluation-of-the-exact-matching-pipeline","title":"Evaluation of the exact matching pipeline\u00b6","text":"<p>Here we evaluate the performance on the task of finding exact matches of source terms to standard concepts in the OHDSI Vocabulary. For this we use a gold standard dataset of source terms with their correct mappings to standard concepts, with some source terms not having any correct mapping.</p> <p></p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#setup","title":"Setup\u00b6","text":"<p>Before running this notebook, make sure the environment is set up as described in the README. This includes credentials for a database with the OHDSI Vocabulary loaded, and credentials for the LLM API. The prompts, as specified in <code>config.yaml</code>, are tailored for GPT o3, so we recommend using that model for this evaluation. This notebook currently assumes the use of a local PGVector instance for vector search, instantiated with the code at https://github.com/schuemie/OhdsiVocabVectorStore, but an option to use the public OHDSI Hecate vector search API is also available.</p> <p>All results will be stored in the <code>data/notebook_results</code> folder. Most code blocks will load results from file if they already exist, to save time and costs. Delete those files to rerun the corresponding steps.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#gold-standard","title":"Gold standard\u00b6","text":"<p>We use the gold standard file in the <code>data/gold_standards</code> folder.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#term-cleanup","title":"Term cleanup\u00b6","text":"<p>The first step is to clean up the source terms, removing any extraneous information that may interfere with matching. This includes removing phrases like \"not otherwise specified\", as well as other uninformative parts of the source term. This step uses the LLM to perform the cleanup. The <code>TermCleaner</code> class handles this, using the system prompts specified in the <code>config.yaml</code> file, and adds a <code>cleaned_term</code> column to the input DataFrame.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#verbatim-matching","title":"Verbatim matching\u00b6","text":"<p>The next step is to perform verbatim matching, i.e. looking for exact matches of the cleaned source terms in the vocabulary. We do allow for some minor variations, such as case differences and punctuation differences.</p> <p>We use the <code>VocabVerbatimTermMapper</code> class for this, which first needs to create an index of the vocabulary terms. For this it will connect to the database specified in the environment variables. It will restrict to the vocabularies and domains specified in the <code>config.yaml</code> file. This adds the <code>mapped_concept_id</code> and <code>mapped_concept_name</code> columns to the input DataFrame.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#embedding-vector-search","title":"Embedding vector search\u00b6","text":"<p>Next, we perform embedding vector search for the source terms that were not matched by verbatim matching. This uses the vector store specified in the <code>config.yaml</code> file, which can be either a local PGVector instance or the OHDSI Hecate API.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#evaluating-the-vector-search-results","title":"Evaluating the vector search results\u00b6","text":"<p>We can now evaluate the vector search results on their own, before combining them with the verbatim matches. The results are written to a text file that contains overall statistics as well as detailed per-term results.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#llm-exact-matching","title":"LLM exact matching\u00b6","text":"<p>Finally, we perform LLM-based exact matching for the source terms that were not matched by verbatim matching, by choosing from a set of candidate concepts retrieved by vector search.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#retrieving-concept-context","title":"Retrieving concept context\u00b6","text":"<p>We observed the LLM does require some context about the target concepts, including the parents, children, and synonyms, to make good mapping decisions. We must therefore retrieve this context from the database before passing it to the LLM.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#llm-mapping","title":"LLM mapping\u00b6","text":"<p>Now we can perform the LLM-based exact matching. This uses the system prompts specified in the <code>config.yaml</code> file, which are tailored for GPT o3. LLM responses are cached to avoid duplicate costs when rerunning.</p>"},{"location":"notebooks/01_evaluate_exact_mapping.html#final-evaluation","title":"Final evaluation\u00b6","text":"<p>Now we can combine the verbatim matches and the LLM-based matches, and evaluate the overall performance.</p>"}]}