{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Ariadne","text":"<p>Ariadne is a Python toolkit for mapping source terminologies to standard concepts in the OHDSI Vocabulary. </p>"},{"location":"#features","title":"Features","text":"<ul> <li>Verbatim term mapping: maps terms that (almost) exactly match standard concepts. Using term normalization techniques like lowercasing, punctuation removal, and word stemming.</li> <li>Embedding vector search: leverages embedding language models to find semantically similar standard concepts for source terms.</li> <li>Exact term mapping: using reasoning LLMs to find exact matches in the vocabulary.</li> <li>Evaluation: uses golden standard mappings to evaluate mapping performance.</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 or higher.</li> </ul>"},{"location":"#install-from-source","title":"Install from Source","text":"<pre><code>git clone [https://github.com/OHDSI/ariadne.git](https://github.com/OHDSI/ariadne.git)\ncd ariadne\n\npip install -e .\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>Ariadne uses environment variables for database connections and API keys.</p> <ol> <li> <p>Copy the example configuration:     <code>bash     cp .env.example .env</code> </p> </li> <li> <p>Edit the <code>.env</code> file to set your database connection details and API keys. Do not commit this file to version control.</p> </li> </ol>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License. See the LICENSE file for details.</p>"},{"location":"autoapi/summary/","title":"Summary","text":"<ul> <li>ariadne<ul> <li>utils<ul> <li>config</li> <li>logging</li> <li>utils</li> </ul> </li> <li>verbatim_mapping<ul> <li>download_terms</li> <li>term_normalizer</li> <li>verbatim_term_mapper</li> <li>vocab_verbatim_term_mapper</li> </ul> </li> </ul> </li> </ul>"},{"location":"autoapi/ariadne/","title":"ariadne","text":""},{"location":"autoapi/ariadne/utils/","title":"utils","text":""},{"location":"autoapi/ariadne/utils/config/","title":"config","text":""},{"location":"autoapi/ariadne/utils/config/#ariadne.utils.config.Config","title":"<code>Config</code>  <code>dataclass</code>","text":"<p>Configuration settings for Ariadne components. By default, loads from 'config.yaml' in the project root or current working directory.</p> Source code in <code>src/ariadne/utils/config.py</code> <pre><code>@dataclass\nclass Config:\n    \"\"\"\n    Configuration settings for Ariadne components. By default, loads from 'config.yaml' in the project root or current\n    working directory.\n    \"\"\"\n\n    log_folder: str\n    terms_folder: str\n    download_batch_size: int\n    verbatim_mapping_index_file: str\n    max_cores: int\n\n    vocabularies: List[str]\n    domain_ids: List[str]\n    include_classification_concepts: bool\n    include_synonyms: bool\n\n    def __init__(self, filename: str = \"config.yaml\"):\n        config = load_config(filename)\n        if config is None:\n            return\n        system = config[\"system\"]\n        for key, value in system.items():\n            setattr(self, key, value)\n        vector_store = config[\"verbatim_mapping\"]\n        for key, value in vector_store.items():\n            setattr(self, key, value)\n\n        self.log_folder = resolve_path(self.log_folder)\n        self.terms_folder = resolve_path(self.terms_folder)\n        self.verbatim_mapping_index_file = resolve_path(\n            self.verbatim_mapping_index_file\n        )\n\n    def __post_init__(self):\n        if self.download_batch_size &lt;= 0:\n            raise ValueError(f\"download_batch_size must be a positive integer\")\n</code></pre>"},{"location":"autoapi/ariadne/utils/logging/","title":"logging","text":""},{"location":"autoapi/ariadne/utils/logging/#ariadne.utils.logging.open_log","title":"<code>open_log(log_file_name, clear_log_file=False)</code>","text":"<p>Sets up the root logger where it writes all logging events to file, and writing events at or above 'info' to console. Events are appended to the log file. The logger will also capture uncaught exceptions.</p> <p>Parameters:</p> Name Type Description Default <code>log_file_name</code> <code>str</code> <p>The name of the file where the log will be written to.</p> required <code>clear_log_file</code> <code>bool</code> <p>If true, the log file will be cleared before writing to it.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/ariadne/utils/logging.py</code> <pre><code>def open_log(log_file_name: str, clear_log_file: bool = False) -&gt; None:\n    \"\"\"\n    Sets up the root logger where it writes all logging events to file, and writing events at or above 'info' to\n    console. Events are appended to the log file. The logger will also capture uncaught exceptions.\n\n    Args:\n        log_file_name: The name of the file where the log will be written to.\n        clear_log_file: If true, the log file will be cleared before writing to it.\n\n    Returns:\n        None\n    \"\"\"\n\n    if clear_log_file:\n        open(log_file_name, \"w\").close()\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    if not len(logger.handlers):\n        _add_file_handler(logger=logger, log_file_name=log_file_name)\n        _add_stream_handler(logger=logger)\n\n    sys.excepthook = _handle_exception\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils/","title":"utils","text":""},{"location":"autoapi/ariadne/utils/utils/#ariadne.utils.utils.get_environment_variable","title":"<code>get_environment_variable(name)</code>","text":"<p>Retrieves the value of the specified environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the environment variable.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The value of the environment variable.</p> <p>Raises:     EnvironmentError: If the environment variable is not set.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def get_environment_variable(name: str) -&gt; str:\n    \"\"\"Retrieves the value of the specified environment variable.\n\n    Args:\n        name: The name of the environment variable.\n\n    Returns:\n        The value of the environment variable.\n    Raises:\n        EnvironmentError: If the environment variable is not set.\n    \"\"\"\n\n    value = os.getenv(name)\n    if value is None:\n        raise EnvironmentError(f\"Environment variable '{name}' is not set.\")\n    return value\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils/#ariadne.utils.utils.get_project_root","title":"<code>get_project_root()</code>","text":"<p>Returns the path to the project root directory.</p> <p>Assumes this file is at src/ariadne/utils/config.py, so the root is 3 levels up.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def get_project_root() -&gt; Path:\n    \"\"\"Returns the path to the project root directory.\n\n    Assumes this file is at src/ariadne/utils/config.py,\n    so the root is 3 levels up.\n    \"\"\"\n\n    return Path(__file__).resolve().parent.parent.parent.parent\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils/#ariadne.utils.utils.resolve_path","title":"<code>resolve_path(path)</code>","text":"<p>If the path is relative, makes it absolute by prepending the project root.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to resolve.</p> required <p>Returns:     The absolute file path as a string.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def resolve_path(path: str) -&gt; str:\n    \"\"\"If the path is relative, makes it absolute by prepending the project root.\n\n    Args:\n        path: The file path to resolve.\n    Returns:\n        The absolute file path as a string.\n    \"\"\"\n\n    p = Path(path)\n    if not p.is_absolute():\n        p = get_project_root() / p\n    return str(p.resolve())\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/","title":"verbatim_mapping","text":""},{"location":"autoapi/ariadne/verbatim_mapping/download_terms/","title":"download_terms","text":""},{"location":"autoapi/ariadne/verbatim_mapping/download_terms/#ariadne.verbatim_mapping.download_terms.download_terms","title":"<code>download_terms(config=Config())</code>","text":"<p>Download terms from vocabulary database and store them in parquet files for use in verbatim mapping. Args:     config: A Config object containing configuration parameters. This function uses the verbatim_mapping section of     the config, which specifies the vocabularies, domains, etc. to filter the terms to be downloaded.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/ariadne/verbatim_mapping/download_terms.py</code> <pre><code>def download_terms(config: Config = Config()) -&gt; None:\n    \"\"\"\n    Download terms from vocabulary database and store them in parquet files for use in verbatim mapping.\n    Args:\n        config: A Config object containing configuration parameters. This function uses the verbatim_mapping section of\n        the config, which specifies the vocabularies, domains, etc. to filter the terms to be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    os.makedirs(config.log_folder, exist_ok=True)\n    os.makedirs(config.terms_folder, exist_ok=True)\n    open_log(os.path.join(config.log_folder, \"logDownloadTerms.txt\"))\n\n    logging.info(\"Starting downloading terms\")\n\n    source_engine = create_engine(get_environment_variable(\"vocab_connection_string\"))\n    query = _create_query(engine=source_engine, config=config)\n\n    with source_engine.connect() as source_connection:\n        terms_result_set = source_connection.execution_options(\n            stream_results=True\n        ).execute(query)\n        total_inserted = 0\n        while True:\n            chunk = terms_result_set.fetchmany(config.download_batch_size)\n            if not chunk:\n                break\n            _store_in_parquet(\n                concept_ids=[row.concept_id for row in chunk],\n                terms=[row.term for row in chunk],\n                concept_names=[row.concept_name for row in chunk],\n                # vocabulary_ids=[row.vocabulary_id for row in chunk],\n                # domain_ids=[row.domain_id for row in chunk],\n                # standard_concepts=[row.standard_concept for row in chunk],\n                # sources=[row.source for row in chunk],\n                file_name=os.path.join(\n                    config.terms_folder,\n                    f\"Terms_{total_inserted + 1}_{total_inserted + len(chunk)}.parquet\",\n                ),\n            )\n            total_inserted += len(chunk)\n            logging.info(\n                f\"Downloaded {len(chunk)} rows, total downloaded: {total_inserted}\"\n            )\n    logging.info(\"Finished downloading terms\")\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer/","title":"term_normalizer","text":""},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer/#ariadne.verbatim_mapping.term_normalizer.TermNormalizer","title":"<code>TermNormalizer</code>","text":"<p>Normalizes clinical term strings for high-precision matching.</p> Source code in <code>src/ariadne/verbatim_mapping/term_normalizer.py</code> <pre><code>class TermNormalizer:\n    \"\"\"\n    Normalizes clinical term strings for high-precision matching.\n    \"\"\"\n\n    def __init__(self):\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n            print(\"spaCy model 'en_core_web_sm' loaded successfully.\")\n        except IOError:\n            print(\"spaCy model 'en_core_web_sm' not found.\")\n            print(\"Please run: python -m spacy download en_core_web_sm\")\n            raise\n\n    def normalize_term(self, term: str) -&gt; str:\n        \"\"\"\n        Normalizes a clinical term string for high-precision matching.\n\n        The pipeline is:\n\n        1. Convert to lowercase.\n        2. Remove possessive \"'s\" at the end of words.\n        3. Remove specific non-informative substrings (e.g., '(disorder)').\n        4. Remove all punctuation.\n        5. Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").\n        6. Join tokens into a single string, preserving order.\n\n        This makes \"liver disorders\" and \"Liver-Disorders (disorder)\"\n        both normalize to \"liver disorder\".\n\n        Args:\n            term: The clinical term string to normalize.\n        Returns:\n            The normalized term string.\n        \"\"\"\n        # 1. Convert to lowercase\n        term = term.lower()\n\n        # 2. Remove possessive 's at the end of a word\n        # This handles \"Alzheimer's disease\" -&gt; \"Alzheimer disease\"\n        # It finds a word character (\\w) followed by 's and a word boundary (\\b),\n        # and replaces the whole thing with just the captured word character (group 1).\n        term = re.sub(r\"(\\w)'s\\b\", r\"\\1\", term)\n\n        # 3. Remove specific non-informative substrings\n        substrings_to_remove = ['(disorder)', '(event)', '(finding)', '(procedure)']\n        for sub in substrings_to_remove:\n            term = term.replace(sub, ' ')\n\n        # 4. Remove all punctuation (replace with a space)\n        # This handles \"liver-disorder\" and \"liver, disorder\"\n        term = re.sub(r'[^\\w\\s]', ' ', term)\n\n        # 5. Tokenize and lemmatize using spaCy\n        doc = self.nlp(term)\n\n        processed_tokens = []\n        for token in doc:\n            # Get the lemma (base form)\n            lemma = token.lemma_\n\n            # 6. Remove empty tokens (from extra spaces)\n            if lemma.strip():\n                processed_tokens.append(lemma)\n\n        # 7. Join tokens into a single string\n        return \" \".join(processed_tokens)\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer/#ariadne.verbatim_mapping.term_normalizer.TermNormalizer.normalize_term","title":"<code>normalize_term(term)</code>","text":"<p>Normalizes a clinical term string for high-precision matching.</p> <p>The pipeline is:</p> <ol> <li>Convert to lowercase.</li> <li>Remove possessive \"'s\" at the end of words.</li> <li>Remove specific non-informative substrings (e.g., '(disorder)').</li> <li>Remove all punctuation.</li> <li>Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").</li> <li>Join tokens into a single string, preserving order.</li> </ol> <p>This makes \"liver disorders\" and \"Liver-Disorders (disorder)\" both normalize to \"liver disorder\".</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>The clinical term string to normalize.</p> required <p>Returns:     The normalized term string.</p> Source code in <code>src/ariadne/verbatim_mapping/term_normalizer.py</code> <pre><code>def normalize_term(self, term: str) -&gt; str:\n    \"\"\"\n    Normalizes a clinical term string for high-precision matching.\n\n    The pipeline is:\n\n    1. Convert to lowercase.\n    2. Remove possessive \"'s\" at the end of words.\n    3. Remove specific non-informative substrings (e.g., '(disorder)').\n    4. Remove all punctuation.\n    5. Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").\n    6. Join tokens into a single string, preserving order.\n\n    This makes \"liver disorders\" and \"Liver-Disorders (disorder)\"\n    both normalize to \"liver disorder\".\n\n    Args:\n        term: The clinical term string to normalize.\n    Returns:\n        The normalized term string.\n    \"\"\"\n    # 1. Convert to lowercase\n    term = term.lower()\n\n    # 2. Remove possessive 's at the end of a word\n    # This handles \"Alzheimer's disease\" -&gt; \"Alzheimer disease\"\n    # It finds a word character (\\w) followed by 's and a word boundary (\\b),\n    # and replaces the whole thing with just the captured word character (group 1).\n    term = re.sub(r\"(\\w)'s\\b\", r\"\\1\", term)\n\n    # 3. Remove specific non-informative substrings\n    substrings_to_remove = ['(disorder)', '(event)', '(finding)', '(procedure)']\n    for sub in substrings_to_remove:\n        term = term.replace(sub, ' ')\n\n    # 4. Remove all punctuation (replace with a space)\n    # This handles \"liver-disorder\" and \"liver, disorder\"\n    term = re.sub(r'[^\\w\\s]', ' ', term)\n\n    # 5. Tokenize and lemmatize using spaCy\n    doc = self.nlp(term)\n\n    processed_tokens = []\n    for token in doc:\n        # Get the lemma (base form)\n        lemma = token.lemma_\n\n        # 6. Remove empty tokens (from extra spaces)\n        if lemma.strip():\n            processed_tokens.append(lemma)\n\n    # 7. Join tokens into a single string\n    return \" \".join(processed_tokens)\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper/","title":"verbatim_term_mapper","text":""},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper/#ariadne.verbatim_mapping.verbatim_term_mapper.VerbatimTermMapper","title":"<code>VerbatimTermMapper</code>","text":"<p>Maps a source term to a provided subset of target concepts based on exact matches of normalized terms.</p> Source code in <code>src/ariadne/verbatim_mapping/verbatim_term_mapper.py</code> <pre><code>class VerbatimTermMapper:\n    \"\"\"\n    Maps a source term to a provided subset of target concepts based on exact matches of normalized terms.\n    \"\"\"\n\n    def __init__(self):\n        self.term_normalizer = TermNormalizer()\n\n    def map_term(\n        self,\n        source_term: str,\n        target_concept_ids: List[int],\n        target_terms: List[str],\n        target_synonyms: List[str],\n    ) -&gt; (Union[int, None], Union[str, None]):\n        \"\"\"\n        Maps a source term to the best matching target concept ID based on normalized terms.\n\n        Args:\n            source_term: the source clinical term to map\n            target_concept_ids: a list of target concept IDs\n            target_terms: a list of target clinical terms\n            target_synonyms: a list of target synonyms. Each string is semicolon separated synonyms for the\n            corresponding target term.\n\n        Returns:\n            A tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)\n        \"\"\"\n        normalized_source = self.term_normalizer.normalize_term(source_term)\n        for concept_id, term, synonyms in zip(\n            target_concept_ids, target_terms, target_synonyms\n        ):\n            normalized_term = self.term_normalizer.normalize_term(term)\n            if normalized_source == normalized_term:\n                return concept_id, term\n            if not pd.isna(synonyms):\n                for synonym in synonyms.split(\";\"):\n                    normalized_synonym = self.term_normalizer.normalize_term(synonym)\n                    if normalized_source == normalized_synonym:\n                        return concept_id, term\n        return None, None\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper/#ariadne.verbatim_mapping.verbatim_term_mapper.VerbatimTermMapper.map_term","title":"<code>map_term(source_term, target_concept_ids, target_terms, target_synonyms)</code>","text":"<p>Maps a source term to the best matching target concept ID based on normalized terms.</p> <p>Parameters:</p> Name Type Description Default <code>source_term</code> <code>str</code> <p>the source clinical term to map</p> required <code>target_concept_ids</code> <code>List[int]</code> <p>a list of target concept IDs</p> required <code>target_terms</code> <code>List[str]</code> <p>a list of target clinical terms</p> required <code>target_synonyms</code> <code>List[str]</code> <p>a list of target synonyms. Each string is semicolon separated synonyms for the</p> required <p>Returns:</p> Type Description <code>(Union[int, None], Union[str, None])</code> <p>A tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)</p> Source code in <code>src/ariadne/verbatim_mapping/verbatim_term_mapper.py</code> <pre><code>def map_term(\n    self,\n    source_term: str,\n    target_concept_ids: List[int],\n    target_terms: List[str],\n    target_synonyms: List[str],\n) -&gt; (Union[int, None], Union[str, None]):\n    \"\"\"\n    Maps a source term to the best matching target concept ID based on normalized terms.\n\n    Args:\n        source_term: the source clinical term to map\n        target_concept_ids: a list of target concept IDs\n        target_terms: a list of target clinical terms\n        target_synonyms: a list of target synonyms. Each string is semicolon separated synonyms for the\n        corresponding target term.\n\n    Returns:\n        A tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)\n    \"\"\"\n    normalized_source = self.term_normalizer.normalize_term(source_term)\n    for concept_id, term, synonyms in zip(\n        target_concept_ids, target_terms, target_synonyms\n    ):\n        normalized_term = self.term_normalizer.normalize_term(term)\n        if normalized_source == normalized_term:\n            return concept_id, term\n        if not pd.isna(synonyms):\n            for synonym in synonyms.split(\";\"):\n                normalized_synonym = self.term_normalizer.normalize_term(synonym)\n                if normalized_source == normalized_synonym:\n                    return concept_id, term\n    return None, None\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper/","title":"vocab_verbatim_term_mapper","text":""},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper/#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.Concept","title":"<code>Concept</code>  <code>dataclass</code>","text":"<p>A data class representing a vocabulary concept with its ID and name.</p> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>@dataclass(slots=True)\nclass Concept:\n    \"\"\"\n    A data class representing a vocabulary concept with its ID and name.\n    \"\"\"\n    concept_id: int\n    concept_name: str\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper/#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.VocabVerbatimTermMapper","title":"<code>VocabVerbatimTermMapper</code>","text":"<p>Maps source terms to concept IDs using a pre-built index of normalized terms. The index is created from vocabulary term files stored in Parquet format, downloaded using the download_terms module. 1. If an index file exists at the verbatim_mapping_index_file path specified in the config, it is loaded. 2. If not, the index is created by processing all Parquet files in the terms folder specified in the config.</p> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>class VocabVerbatimTermMapper:\n    \"\"\"\n    Maps source terms to concept IDs using a pre-built index of normalized terms.\n    The index is created from vocabulary term files stored in Parquet format, downloaded using the download_terms\n    module.\n    1. If an index file exists at the verbatim_mapping_index_file path specified in the config, it is loaded.\n    2. If not, the index is created by processing all Parquet files in the terms folder specified in the config.\n    \"\"\"\n\n    def __init__(self, config: Config = Config()):\n        self.term_normalizer = TermNormalizer()\n        if os.path.exists(config.verbatim_mapping_index_file):\n            with open(config.verbatim_mapping_index_file, \"rb\") as handle:\n                self.index = pickle.load(handle)\n            print(f\"Index loaded from {config.verbatim_mapping_index_file}\")\n        else:\n            self._create_index(config)\n\n    def _create_index(self, config: Config):\n        print(\"Creating index\")\n        if not os.path.exists(config.terms_folder):\n            raise FileNotFoundError(\n                f\"Terms folder {config.terms_folder} does not exist. Make sure to run the download_terms module first.\"\n            )\n        all_files = [\n            os.path.join(config.terms_folder, f)\n            for f in os.listdir(config.terms_folder)\n            if f.endswith(\".parquet\")\n        ]\n        pool = multiprocessing.get_context(\"spawn\").Pool(processes=config.max_cores)\n        index_data = {}\n        for file in all_files:\n            print(f\"Processing file: {file}\")\n            df = pd.read_parquet(file)\n            normalized_terms = pool.map(\n                self.term_normalizer.normalize_term, df[\"term\"].tolist()\n            )\n            for norm_term, concept_id, concept_name in zip(\n                normalized_terms, df[\"concept_id\"].tolist(), df[\"concept_name\"].tolist()\n            ):\n                concept = Concept(concept_id, concept_name)\n                if norm_term in index_data:\n                    existing = index_data[norm_term]\n                    if isinstance(existing, list):\n                        if concept_id not in existing:\n                            existing.append(concept)\n                    else:\n                        if concept_id != existing:\n                            index_data[norm_term] = [existing, concept]\n                else:\n                    index_data[norm_term] = concept\n\n        pool.close()\n        self.index = index_data\n\n        try:\n            with open(config.verbatim_mapping_index_file, \"wb\") as f:\n                pickle.dump(index_data, f)\n            print(f\"Index saved to {config.verbatim_mapping_index_file}\")\n        except OSError as e:\n            print(f\"Error saving index: {e}\")\n\n    def map_term(self, source_term: str) -&gt; List[Concept]:\n        \"\"\"\n        Maps a source term to concept IDs using the pre-built index.\n\n        Args:\n            source_term: the source clinical term to map\n\n        Returns:\n            A list of matching concepts, possibly empty if no match is found.\n        \"\"\"\n        normalized_source = self.term_normalizer.normalize_term(source_term)\n        if normalized_source in self.index:\n            concept_ids = self.index[normalized_source]\n            if isinstance(concept_ids, list):\n                return concept_ids\n            else:\n                return [concept_ids]\n        return []\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper/#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.VocabVerbatimTermMapper.map_term","title":"<code>map_term(source_term)</code>","text":"<p>Maps a source term to concept IDs using the pre-built index.</p> <p>Parameters:</p> Name Type Description Default <code>source_term</code> <code>str</code> <p>the source clinical term to map</p> required <p>Returns:</p> Type Description <code>List[Concept]</code> <p>A list of matching concepts, possibly empty if no match is found.</p> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>def map_term(self, source_term: str) -&gt; List[Concept]:\n    \"\"\"\n    Maps a source term to concept IDs using the pre-built index.\n\n    Args:\n        source_term: the source clinical term to map\n\n    Returns:\n        A list of matching concepts, possibly empty if no match is found.\n    \"\"\"\n    normalized_source = self.term_normalizer.normalize_term(source_term)\n    if normalized_source in self.index:\n        concept_ids = self.index[normalized_source]\n        if isinstance(concept_ids, list):\n            return concept_ids\n        else:\n            return [concept_ids]\n    return []\n</code></pre>"}]}