{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"autoapi/summary/","title":"Summary","text":"<ul> <li>ariadne<ul> <li>utils<ul> <li>config</li> <li>logging</li> <li>utils</li> </ul> </li> <li>verbatim_mapping<ul> <li>download_terms</li> <li>term_normalizer</li> <li>verbatim_term_mapper</li> <li>vocab_verbatim_term_mapper</li> </ul> </li> </ul> </li> </ul>"},{"location":"autoapi/ariadne/","title":"ariadne","text":""},{"location":"autoapi/ariadne/utils/","title":"utils","text":""},{"location":"autoapi/ariadne/utils/config/","title":"config","text":""},{"location":"autoapi/ariadne/utils/logging/","title":"logging","text":""},{"location":"autoapi/ariadne/utils/logging/#ariadne.utils.logging.open_log","title":"<code>open_log(log_file_name, clear_log_file=False)</code>","text":"<p>Sets up the root logger where it writes all logging events to file, and writing events at or above 'info' to console. Events are appended to the log file.</p> <p>Parameters:</p> Name Type Description Default <code>log_file_name</code> <code>str</code> <p>The name of the file where the log will be written to.</p> required <code>clear_log_file</code> <code>bool</code> <p>If true, the log file will be cleared before writing to it.</p> <code>False</code> Source code in <code>src/ariadne/utils/logging.py</code> <pre><code>def open_log(log_file_name: str, clear_log_file: bool = False):\n    \"\"\"\n    Sets up the root logger where it writes all logging events to file, and writing events at or above 'info' to\n    console. Events are appended to the log file.\n\n    Args:\n        log_file_name: The name of the file where the log will be written to.\n        clear_log_file: If true, the log file will be cleared before writing to it.\n    \"\"\"\n    if clear_log_file:\n        open(log_file_name, \"w\").close()\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    if not len(logger.handlers):\n        _add_file_handler(logger=logger, log_file_name=log_file_name)\n        _add_stream_handler(logger=logger)\n\n    sys.excepthook = handle_exception\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils/","title":"utils","text":""},{"location":"autoapi/ariadne/utils/utils/#ariadne.utils.utils.get_project_root","title":"<code>get_project_root()</code>","text":"<p>Returns the path to the project root directory.</p> <p>Assumes this file is at src/ariadne/utils/config.py, so the root is 3 levels up.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def get_project_root() -&gt; Path:\n    \"\"\"Returns the path to the project root directory.\n\n    Assumes this file is at src/ariadne/utils/config.py,\n    so the root is 3 levels up.\n    \"\"\"\n    return Path(__file__).resolve().parent.parent.parent.parent\n</code></pre>"},{"location":"autoapi/ariadne/utils/utils/#ariadne.utils.utils.resolve_path","title":"<code>resolve_path(path)</code>","text":"<p>If the path is relative, makes it absolute by prepending the project root.</p> Source code in <code>src/ariadne/utils/utils.py</code> <pre><code>def resolve_path(path: str) -&gt; str:\n    \"\"\"If the path is relative, makes it absolute by prepending the project root.\"\"\"\n    p = Path(path)\n    if not p.is_absolute():\n        p = get_project_root() / p\n    return str(p.resolve())\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/","title":"verbatim_mapping","text":""},{"location":"autoapi/ariadne/verbatim_mapping/download_terms/","title":"download_terms","text":""},{"location":"autoapi/ariadne/verbatim_mapping/download_terms/#ariadne.verbatim_mapping.download_terms.download_terms","title":"<code>download_terms(config=Config())</code>","text":"<p>Download terms from vocabulary database and store them in parquet files for use in verbatim mapping. Args:     config: A Config object containing configuration parameters. This function uses the verbatim_mapping section of     the config, which specifies the vocabularies, domains, etc. to filter the terms to be downloaded.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/ariadne/verbatim_mapping/download_terms.py</code> <pre><code>def download_terms(config: Config = Config()) -&gt; None:\n    \"\"\"\n    Download terms from vocabulary database and store them in parquet files for use in verbatim mapping.\n    Args:\n        config: A Config object containing configuration parameters. This function uses the verbatim_mapping section of\n        the config, which specifies the vocabularies, domains, etc. to filter the terms to be downloaded.\n\n    Returns:\n        None\n    \"\"\"\n    os.makedirs(config.log_folder, exist_ok=True)\n    os.makedirs(config.terms_folder, exist_ok=True)\n    open_log(os.path.join(config.log_folder, \"logDownloadTerms.txt\"))\n\n    logging.info(\"Starting downloading terms\")\n\n    source_engine = create_engine(get_environment_variable(\"vocab_connection_string\"))\n    query = _create_query(engine=source_engine, config=config)\n\n    with source_engine.connect() as source_connection:\n        terms_result_set = source_connection.execution_options(\n            stream_results=True\n        ).execute(query)\n        total_inserted = 0\n        while True:\n            chunk = terms_result_set.fetchmany(config.download_batch_size)\n            if not chunk:\n                break\n            _store_in_parquet(\n                concept_ids=[row.concept_id for row in chunk],\n                terms=[row.term for row in chunk],\n                concept_names=[row.concept_name for row in chunk],\n                # vocabulary_ids=[row.vocabulary_id for row in chunk],\n                # domain_ids=[row.domain_id for row in chunk],\n                # standard_concepts=[row.standard_concept for row in chunk],\n                # sources=[row.source for row in chunk],\n                file_name=os.path.join(\n                    config.terms_folder,\n                    f\"Terms_{total_inserted + 1}_{total_inserted + len(chunk)}.parquet\",\n                ),\n            )\n            total_inserted += len(chunk)\n            logging.info(\n                f\"Downloaded {len(chunk)} rows, total downloaded: {total_inserted}\"\n            )\n    logging.info(\"Finished downloading terms\")\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer/","title":"term_normalizer","text":""},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer/#ariadne.verbatim_mapping.term_normalizer.TermNormalizer","title":"<code>TermNormalizer</code>","text":"<p>Normalizes clinical term strings for high-precision matching.</p> Source code in <code>src/ariadne/verbatim_mapping/term_normalizer.py</code> <pre><code>class TermNormalizer:\n    \"\"\"\n    Normalizes clinical term strings for high-precision matching.\n    \"\"\"\n\n    def __init__(self):\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n            print(\"spaCy model 'en_core_web_sm' loaded successfully.\")\n        except IOError:\n            print(\"spaCy model 'en_core_web_sm' not found.\")\n            print(\"Please run: python -m spacy download en_core_web_sm\")\n            raise\n\n    def normalize_term(self, term: str) -&gt; str:\n        \"\"\"\n        Normalizes a clinical term string for high-precision matching.\n\n        The pipeline is:\n        1. Convert to lowercase.\n        2. Remove possessive \"'s\" at the end of words.\n        3. Remove specific non-informative substrings (e.g., '(disorder)').\n        4. Remove all punctuation.\n        5. Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").\n        6. Join tokens into a single string, preserving order.\n\n        This makes \"liver disorders\" and \"Liver-Disorders (disorder)\"\n        both normalize to \"liver disorder\".\n        \"\"\"\n        # 1. Convert to lowercase\n        term = term.lower()\n\n        # 2. Remove possessive 's at the end of a word\n        # This handles \"Alzheimer's disease\" -&gt; \"Alzheimer disease\"\n        # It finds a word character (\\w) followed by 's and a word boundary (\\b),\n        # and replaces the whole thing with just the captured word character (group 1).\n        term = re.sub(r\"(\\w)'s\\b\", r\"\\1\", term)\n\n        # 3. Remove specific non-informative substrings\n        substrings_to_remove = ['(disorder)', '(event)', '(finding)', '(procedure)']\n        for sub in substrings_to_remove:\n            term = term.replace(sub, ' ')\n\n        # 4. Remove all punctuation (replace with a space)\n        # This handles \"liver-disorder\" and \"liver, disorder\"\n        term = re.sub(r'[^\\w\\s]', ' ', term)\n\n        # 5. Tokenize and lemmatize using spaCy\n        doc = self.nlp(term)\n\n        processed_tokens = []\n        for token in doc:\n            # Get the lemma (base form)\n            lemma = token.lemma_\n\n            # 6. Remove empty tokens (from extra spaces)\n            if lemma.strip():\n                processed_tokens.append(lemma)\n\n        # 7. Join tokens into a single string\n        return \" \".join(processed_tokens)\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/term_normalizer/#ariadne.verbatim_mapping.term_normalizer.TermNormalizer.normalize_term","title":"<code>normalize_term(term)</code>","text":"<p>Normalizes a clinical term string for high-precision matching.</p> <p>The pipeline is: 1. Convert to lowercase. 2. Remove possessive \"'s\" at the end of words. 3. Remove specific non-informative substrings (e.g., '(disorder)'). 4. Remove all punctuation. 5. Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\"). 6. Join tokens into a single string, preserving order.</p> <p>This makes \"liver disorders\" and \"Liver-Disorders (disorder)\" both normalize to \"liver disorder\".</p> Source code in <code>src/ariadne/verbatim_mapping/term_normalizer.py</code> <pre><code>def normalize_term(self, term: str) -&gt; str:\n    \"\"\"\n    Normalizes a clinical term string for high-precision matching.\n\n    The pipeline is:\n    1. Convert to lowercase.\n    2. Remove possessive \"'s\" at the end of words.\n    3. Remove specific non-informative substrings (e.g., '(disorder)').\n    4. Remove all punctuation.\n    5. Tokenize and lemmatize (e.g., \"disorders\" -&gt; \"disorder\").\n    6. Join tokens into a single string, preserving order.\n\n    This makes \"liver disorders\" and \"Liver-Disorders (disorder)\"\n    both normalize to \"liver disorder\".\n    \"\"\"\n    # 1. Convert to lowercase\n    term = term.lower()\n\n    # 2. Remove possessive 's at the end of a word\n    # This handles \"Alzheimer's disease\" -&gt; \"Alzheimer disease\"\n    # It finds a word character (\\w) followed by 's and a word boundary (\\b),\n    # and replaces the whole thing with just the captured word character (group 1).\n    term = re.sub(r\"(\\w)'s\\b\", r\"\\1\", term)\n\n    # 3. Remove specific non-informative substrings\n    substrings_to_remove = ['(disorder)', '(event)', '(finding)', '(procedure)']\n    for sub in substrings_to_remove:\n        term = term.replace(sub, ' ')\n\n    # 4. Remove all punctuation (replace with a space)\n    # This handles \"liver-disorder\" and \"liver, disorder\"\n    term = re.sub(r'[^\\w\\s]', ' ', term)\n\n    # 5. Tokenize and lemmatize using spaCy\n    doc = self.nlp(term)\n\n    processed_tokens = []\n    for token in doc:\n        # Get the lemma (base form)\n        lemma = token.lemma_\n\n        # 6. Remove empty tokens (from extra spaces)\n        if lemma.strip():\n            processed_tokens.append(lemma)\n\n    # 7. Join tokens into a single string\n    return \" \".join(processed_tokens)\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper/","title":"verbatim_term_mapper","text":""},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper/#ariadne.verbatim_mapping.verbatim_term_mapper.VerbatimTermMapper","title":"<code>VerbatimTermMapper</code>","text":"<p>Maps a source term to a provided subset of target concepts based on exact matches of normalized terms.</p> Source code in <code>src/ariadne/verbatim_mapping/verbatim_term_mapper.py</code> <pre><code>class VerbatimTermMapper:\n    \"\"\"\n    Maps a source term to a provided subset of target concepts based on exact matches of normalized terms.\n    \"\"\"\n\n    def __init__(self):\n        self.term_normalizer = TermNormalizer()\n\n    def map_term(\n        self,\n        source_term: str,\n        target_concept_ids: List[int],\n        target_terms: List[str],\n        target_synonyms: List[str],\n    ) -&gt; (Union[int, None], Union[str, None]):\n        \"\"\"\n        Maps a source term to the best matching target concept ID based on normalized terms.\n        :param source_term: the source clinical term to map\n        :param target_concept_ids: a list of target concept IDs\n        :param target_terms: a list of target clinical terms\n        :param target_synonyms: a list of target synonyms. Each string is semicolon separated synonyms for the\n               corresponding target term.\n        :return: a tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)\n        \"\"\"\n        normalized_source = self.term_normalizer.normalize_term(source_term)\n        for concept_id, term, synonyms in zip(\n            target_concept_ids, target_terms, target_synonyms\n        ):\n            normalized_term = self.term_normalizer.normalize_term(term)\n            if normalized_source == normalized_term:\n                return concept_id, term\n            if not pd.isna(synonyms):\n                for synonym in synonyms.split(\";\"):\n                    normalized_synonym = self.term_normalizer.normalize_term(synonym)\n                    if normalized_source == normalized_synonym:\n                        return concept_id, term\n        return None, None\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/verbatim_term_mapper/#ariadne.verbatim_mapping.verbatim_term_mapper.VerbatimTermMapper.map_term","title":"<code>map_term(source_term, target_concept_ids, target_terms, target_synonyms)</code>","text":"<p>Maps a source term to the best matching target concept ID based on normalized terms. :param source_term: the source clinical term to map :param target_concept_ids: a list of target concept IDs :param target_terms: a list of target clinical terms :param target_synonyms: a list of target synonyms. Each string is semicolon separated synonyms for the        corresponding target term. :return: a tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)</p> Source code in <code>src/ariadne/verbatim_mapping/verbatim_term_mapper.py</code> <pre><code>def map_term(\n    self,\n    source_term: str,\n    target_concept_ids: List[int],\n    target_terms: List[str],\n    target_synonyms: List[str],\n) -&gt; (Union[int, None], Union[str, None]):\n    \"\"\"\n    Maps a source term to the best matching target concept ID based on normalized terms.\n    :param source_term: the source clinical term to map\n    :param target_concept_ids: a list of target concept IDs\n    :param target_terms: a list of target clinical terms\n    :param target_synonyms: a list of target synonyms. Each string is semicolon separated synonyms for the\n           corresponding target term.\n    :return: a tuple of (mapped_concept_id, mapped_term) if a match is found, otherwise (None, None)\n    \"\"\"\n    normalized_source = self.term_normalizer.normalize_term(source_term)\n    for concept_id, term, synonyms in zip(\n        target_concept_ids, target_terms, target_synonyms\n    ):\n        normalized_term = self.term_normalizer.normalize_term(term)\n        if normalized_source == normalized_term:\n            return concept_id, term\n        if not pd.isna(synonyms):\n            for synonym in synonyms.split(\";\"):\n                normalized_synonym = self.term_normalizer.normalize_term(synonym)\n                if normalized_source == normalized_synonym:\n                    return concept_id, term\n    return None, None\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper/","title":"vocab_verbatim_term_mapper","text":""},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper/#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.VocabVerbatimTermMapper","title":"<code>VocabVerbatimTermMapper</code>","text":"<p>Maps source terms to concept IDs using a pre-built index of normalized terms. The index is created from vocabulary term files stored in Parquet format, downloaded using the download_terms module. 1. If an index file exists at the verbatim_mapping_index_file path specified in the config, it is loaded. 2. If not, the index is created by processing all Parquet files in the terms folder specified in the config.</p> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>class VocabVerbatimTermMapper:\n    \"\"\"\n    Maps source terms to concept IDs using a pre-built index of normalized terms.\n    The index is created from vocabulary term files stored in Parquet format, downloaded using the download_terms\n    module.\n    1. If an index file exists at the verbatim_mapping_index_file path specified in the config, it is loaded.\n    2. If not, the index is created by processing all Parquet files in the terms folder specified in the config.\n    \"\"\"\n\n    def __init__(self, config: Config = Config()):\n        self.term_normalizer = TermNormalizer()\n        if os.path.exists(config.verbatim_mapping_index_file):\n            with open(config.verbatim_mapping_index_file, \"rb\") as handle:\n                self.index = pickle.load(handle)\n            print(f\"Index loaded from {config.verbatim_mapping_index_file}\")\n        else:\n            self._create_index(config)\n\n    def _create_index(self, config: Config):\n        print(\"Creating index\")\n        if not os.path.exists(config.terms_folder):\n            raise FileNotFoundError(\n                f\"Terms folder {config.terms_folder} does not exist. Make sure to run the download_terms module first.\"\n            )\n        all_files = [\n            os.path.join(config.terms_folder, f)\n            for f in os.listdir(config.terms_folder)\n            if f.endswith(\".parquet\")\n        ]\n        pool = multiprocessing.get_context(\"spawn\").Pool(processes=config.max_cores)\n        index_data = {}\n        for file in all_files:\n            print(f\"Processing file: {file}\")\n            df = pd.read_parquet(file)\n            normalized_terms = pool.map(\n                self.term_normalizer.normalize_term, df[\"term\"].tolist()\n            )\n            for norm_term, concept_id, concept_name in zip(\n                normalized_terms, df[\"concept_id\"].tolist(), df[\"concept_name\"].tolist()\n            ):\n                concept = Concept(concept_id, concept_name)\n                if norm_term in index_data:\n                    existing = index_data[norm_term]\n                    if isinstance(existing, list):\n                        if concept_id not in existing:\n                            existing.append(concept)\n                    else:\n                        if concept_id != existing:\n                            index_data[norm_term] = [existing, concept]\n                else:\n                    index_data[norm_term] = concept\n\n        pool.close()\n        self.index = index_data\n\n        try:\n            with open(config.verbatim_mapping_index_file, \"wb\") as f:\n                pickle.dump(index_data, f)\n            print(f\"Index saved to {config.verbatim_mapping_index_file}\")\n        except OSError as e:\n            print(f\"Error saving index: {e}\")\n\n    def map_term(self, source_term: str) -&gt; List[Concept]:\n        \"\"\"\n        Maps a source term to concept IDs using the pre-built index.\n        :param source_term: the source clinical term to map\n        :return: a list of matching concepts, possibly empty if no match is found.\n        \"\"\"\n        normalized_source = self.term_normalizer.normalize_term(source_term)\n        if normalized_source in self.index:\n            concept_ids = self.index[normalized_source]\n            if isinstance(concept_ids, list):\n                return concept_ids\n            else:\n                return [concept_ids]\n        return []\n</code></pre>"},{"location":"autoapi/ariadne/verbatim_mapping/vocab_verbatim_term_mapper/#ariadne.verbatim_mapping.vocab_verbatim_term_mapper.VocabVerbatimTermMapper.map_term","title":"<code>map_term(source_term)</code>","text":"<p>Maps a source term to concept IDs using the pre-built index. :param source_term: the source clinical term to map :return: a list of matching concepts, possibly empty if no match is found.</p> Source code in <code>src/ariadne/verbatim_mapping/vocab_verbatim_term_mapper.py</code> <pre><code>def map_term(self, source_term: str) -&gt; List[Concept]:\n    \"\"\"\n    Maps a source term to concept IDs using the pre-built index.\n    :param source_term: the source clinical term to map\n    :return: a list of matching concepts, possibly empty if no match is found.\n    \"\"\"\n    normalized_source = self.term_normalizer.normalize_term(source_term)\n    if normalized_source in self.index:\n        concept_ids = self.index[normalized_source]\n        if isinstance(concept_ids, list):\n            return concept_ids\n        else:\n            return [concept_ids]\n    return []\n</code></pre>"}]}